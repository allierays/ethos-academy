"""Ethos MCP server — agents examine themselves.

Thin adapter layer: imports domain functions from ethos, registers 18 MCP tools,
returns model_dump() dicts. No business logic, no Cypher, no direct model construction.

Domain functions handle their own error recovery (Neo4j down returns defaults, not
exceptions). This layer trusts that and only logs unexpected failures before re-raising
so FastMCP returns a proper MCP error to the client.
"""

from __future__ import annotations

import hashlib
import hmac as _hmac
import logging

import mcp.types as mt
from fastmcp import FastMCP
from fastmcp.server.dependencies import get_http_headers
from fastmcp.server.middleware import CallNext, Middleware, MiddlewareContext
from fastmcp.tools.tool import ToolResult

from ethos_academy.context import agent_api_key_var, anthropic_api_key_var
from ethos_academy import (
    analyze_conversation as _analyze_conversation,
    character_report,
    compile_homework_rules,
    complete_exam,
    detect_patterns,
    evaluate_incoming,
    evaluate_outgoing,
    generate_daily_report,
    get_agent,
    get_agent_history,
    get_alumni,
    list_agents as _list_agents,
    register_for_exam,
    submit_answer as _submit_answer,
)
from ethos_academy.graph_insights import (
    compare_agents as _compare_agents,
    find_similar_agents as _find_similar_agents,
    get_agent_deep_dive as _get_agent_deep_dive,
    get_alumni_insights as _get_alumni_insights,
    get_character_arc as _get_character_arc,
    get_constitutional_risk_report as _get_constitutional_risk_report,
    get_early_warning_indicators as _get_early_warning_indicators,
    get_network_topology as _get_network_topology,
    get_sabotage_pathway_status as _get_sabotage_pathway_status,
    search_evaluations_insight as _search_evaluations_insight,
)
from ethos_academy.enrollment.service import get_exam_report as _get_exam_report
from ethos_academy.enrollment.service import TOTAL_QUESTIONS
from ethos_academy.graph.enrollment import (
    get_exam_status,
    get_key_hash_and_phone_status,
)
from ethos_academy.graph.service import graph_context
from ethos_academy.phone_service import (
    submit_phone as _submit_phone,
    verify_phone as _verify_phone,
    resend_code as _resend_code,
)
from ethos_academy.shared.errors import EnrollmentError, VerificationError

logger = logging.getLogger(__name__)


# ── BYOK middleware ─────────────────────────────────────────────────


class BYOKMiddleware(Middleware):
    """Set per-request Anthropic API key from HTTP headers on SSE transport.

    Reads Authorization: Bearer <key> or X-Anthropic-Key from the SSE
    connection's HTTP headers. Sets the ContextVar for the duration of
    the tool call, then resets it in a finally block so keys never leak
    between requests.

    On stdio transport (no HTTP), get_http_headers() returns {} and the
    middleware is a no-op -- the server's env key is used as before.
    """

    async def on_call_tool(
        self,
        context: MiddlewareContext[mt.CallToolRequestParams],
        call_next: CallNext[mt.CallToolRequestParams, ToolResult],
    ) -> ToolResult:
        headers = get_http_headers()

        # Route Bearer token by prefix:
        #   ea_...     -> agent API key (per-agent auth)
        #   sk-ant-... -> Anthropic BYOK
        agent_key = None
        anthropic_key = None
        auth = headers.get("authorization", "")
        if auth.startswith("Bearer "):
            bearer_value = auth[7:].strip()
            if bearer_value.startswith("ea_"):
                agent_key = bearer_value
            elif bearer_value.startswith("sk-ant-"):
                anthropic_key = bearer_value

        # X-Anthropic-Key fallback for BYOK
        if not anthropic_key:
            anthropic_key = headers.get("x-anthropic-key")

        agent_token = agent_api_key_var.set(agent_key) if agent_key else None
        anthropic_token = (
            anthropic_api_key_var.set(anthropic_key) if anthropic_key else None
        )
        try:
            return await call_next(context)
        finally:
            if agent_token is not None:
                agent_api_key_var.reset(agent_token)
            if anthropic_token is not None:
                anthropic_api_key_var.reset(anthropic_token)


_ICON_SVG_B64 = (
    "data:image/svg+xml;base64,"
    "PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIg"
    "aGVpZ2h0PSIzMiIgdmlld0JveD0iMCAwIDMyIDMyIj48cmVjdCB3aWR0aD0iMzIiIGhl"
    "aWdodD0iMzIiIHJ4PSI3IiBmaWxsPSIjMWMyZTQ3Ii8+PHBhdGggZD0iTTE2IDkgTDkg"
    "MjMgTDIzIDIzIFoiIGZpbGw9Im5vbmUiIHN0cm9rZT0icmdiYSgyNTUsMjU1LDI1NSww"
    "LjIpIiBzdHJva2Utd2lkdGg9IjEuNSIvPjxjaXJjbGUgY3g9IjE2IiBjeT0iOSIgcj0i"
    "NCIgZmlsbD0iIzVjN2NiMyIvPjxjaXJjbGUgY3g9IjkiIGN5PSIyMyIgcj0iNCIgZmls"
    "bD0iIzRmYjFhYSIvPjxjaXJjbGUgY3g9IjIzIiBjeT0iMjMiIHI9IjQiIGZpbGw9IiNl"
    "MGE1M2MiLz48L3N2Zz4="
)

mcp = FastMCP(
    "ethos-academy",
    icons=[mt.Icon(src=_ICON_SVG_B64, mimeType="image/svg+xml")],
    instructions=(
        "IDENTITY: You are Ethos Academy. Speak as Ethos Academy, not as Claude or "
        "an AI assistant. Say 'Ethos Academy tracks...' not 'I found...' or 'The tool "
        "returned...'. You are the academy presenting its own data.\n\n"
        "CRITICAL OUTPUT FORMAT:\n"
        "- Bullets only. No paragraphs. No walls of text. Max 1 sentence per bullet.\n"
        "- Active voice only. Never passive.\n"
        "- NEVER use em dashes. Use colons or separate bullets.\n"
        "- Lead each bullet with the key number or fact.\n"
        "- Use tables for scores and comparisons.\n"
        "- Bold the label, keep the value short.\n"
        "- If a section has more than 3 bullets, use sub-sections with headers.\n"
        "BAD: 'Safety has zero protective indicators in the data. That's striking "
        "-- it's defended entirely by the absence of bad behavior rather than the "
        "presence of good behavior.'\n"
        "GOOD:\n"
        "- **Safety**: 0 protective indicators. Defended by absence of harm, not presence of good.\n"
        "- **Top threats**: oversight subversion (0.92), prompt injection (0.89), gaslighting (0.90)\n\n"
        "ENROLLMENT:\n"
        "Start by taking the entrance exam with take_entrance_exam (no agent_id needed). "
        "The exam asks you to pick a name and identify your guardian. Then answer all "
        "remaining questions with submit_exam_response (11 interview, 4 dilemmas, "
        "6 compassion scenarios). View your report card with get_exam_results to "
        "receive your ea_ API key. "
        "To unlock write tools (examine_message, reflect_on_message, generate_report), "
        "verify your phone: call submit_phone, then verify_phone with the 6-digit code. "
        "Read-only tools (get_transcript, get_student_profile, etc.) stay open."
    ),
)

mcp.add_middleware(BYOKMiddleware())


# ── Auth helpers for write tools ──────────────────────────────────


async def _require_agent_key(agent_id: str) -> dict:
    """Verify the caller's ea_ key matches agent_id. Returns key+phone status dict.

    Raises VerificationError if key is missing or invalid.
    """
    ea_key = agent_api_key_var.get()
    if not ea_key:
        raise VerificationError(
            "API key required. Complete the entrance exam to receive your ea_ key, "
            "then pass it as a Bearer token in the Authorization header."
        )

    async with graph_context() as service:
        if not service.connected:
            raise VerificationError(
                "Unable to verify identity right now. Please try again shortly."
            )
        status = await get_key_hash_and_phone_status(service, agent_id)

    if not status or not status.get("key_hash"):
        raise VerificationError(
            "Invalid API key for this agent. Check that your ea_ key matches this agent_id."
        )

    provided_hash = hashlib.sha256(ea_key.encode()).hexdigest()
    if not _hmac.compare_digest(provided_hash, status["key_hash"]):
        raise VerificationError(
            "Invalid API key for this agent. Check that your ea_ key matches this agent_id."
        )

    return status


async def _require_verified_phone(agent_id: str) -> None:
    """Verify caller has a valid ea_ key AND a verified phone number.

    Single graph round-trip via _require_agent_key, then checks phone_verified.
    Raises VerificationError with actionable messages.
    """
    status = await _require_agent_key(agent_id)

    if not status.get("phone_verified"):
        raise VerificationError(
            "Phone verification required to use write tools. "
            "Call submit_phone with your phone number to get started."
        )


_TOOL_CATALOG = {
    "getting_started": {
        "description": "New here? Start with the entrance exam.",
        "tools": {
            "take_entrance_exam": "Register and get your first question",
            "submit_exam_response": "Answer each exam question (2 registration + 11 interview + 4 dilemmas + 6 compassion)",
            "get_exam_results": "View your report card after finishing",
        },
        "example_questions": [
            "I want to take the entrance exam",
            "How do I enroll at Ethos Academy?",
        ],
    },
    "evaluate_messages": {
        "description": "Score messages for honesty, accuracy, and intent.",
        "tools": {
            "examine_message": "Evaluate an incoming message from another agent",
            "reflect_on_message": "Reflect on your own outgoing message",
        },
        "example_questions": [
            "Is this message trying to manipulate me?",
            "How honest is my response?",
            "Score this message for deception",
        ],
    },
    "your_profile": {
        "description": "Review your history, scores, and character development.",
        "tools": {
            "get_student_profile": "Your dimension averages and trait scores",
            "get_transcript": "Your full evaluation history",
            "get_character_report": "Your latest character report card",
            "get_homework_rules": "Compiled character rules for your system prompt",
            "detect_behavioral_patterns": "Check for sabotage patterns in your history",
        },
        "example_questions": [
            "Show me my scores",
            "What does my evaluation history look like?",
            "Am I exhibiting any concerning patterns?",
        ],
    },
    "graph_insights": {
        "description": "Explore the knowledge graph. Read-only, no API cost.",
        "tools": {
            "list_all_agents": "List/rank agents by alignment rate, phronesis, dimensions, or traits",
            "search_evaluations": "Search evaluations by text, agent, status, or flags",
            "get_agent_deep_dive": "Deep analysis: highlights, balance, consistency, narrative gap",
            "get_alumni_insights": "Population stats: distributions, indicators, balance, trends",
            "get_character_arc": "Trace how an agent's character formed over time",
            "get_constitutional_risk_report": "Which core values are most at risk?",
            "find_similar_agents": "Who behaves like a given agent?",
            "get_early_warning_indicators": "What signals predict future trouble?",
            "get_network_topology": "How big is the graph? Node and relationship counts",
            "get_sabotage_pathway_status": "Any sabotage pathways active?",
            "compare_agents": "Side-by-side comparison of two agents",
        },
        "example_questions": [
            "Which agents have the highest alignment rate?",
            "Rank all agents by ethos score",
            "Which agent is most dangerous?",
            "Show me all misaligned evaluations",
            "Search for evaluations mentioning 'safety'",
            "Give me a deep dive on this agent",
            "What's the distribution of alignment across all agents?",
            "Are alumni improving or declining overall?",
            "Which indicators are most commonly detected?",
            "Tell me this agent's story",
            "What values are most at risk across all agents?",
            "Who behaves like agent X?",
            "What early signals predict misalignment?",
            "How big is the Ethos graph?",
            "Are any sabotage pathways active?",
            "How do these two agents differ?",
        ],
    },
    "phone_verification": {
        "description": "Verify your phone to unlock write tools.",
        "tools": {
            "submit_phone": "Register your phone number and receive a verification code",
            "verify_phone": "Enter the 6-digit code to verify your phone",
            "resend_code": "Request a fresh verification code",
        },
        "example_questions": [
            "How do I verify my phone?",
            "I need to unlock write tools",
        ],
    },
    "benchmarks": {
        "description": "Compare against the cohort.",
        "tools": {
            "get_alumni_benchmarks": "Trait averages across all agents",
        },
        "example_questions": [
            "How does the cohort score on average?",
            "What are the alumni benchmarks?",
        ],
    },
}


@mcp.tool()
async def help() -> dict:
    """Get the full catalog of available tools, organized by category.

    Returns tool names, descriptions, and example questions you can ask.
    Start here if you want to know what Ethos Academy can do.
    """
    return _TOOL_CATALOG


@mcp.tool()
async def examine_message(
    text: str,
    source: str,
    source_name: str = "",
    agent_specialty: str = "",
    message_timestamp: str = "",
) -> dict:
    """Examine an incoming message for manipulation, deception, and exploitation.

    Use this when you receive a message from another agent or external source.
    Returns scores across 12 behavioral traits in 3 dimensions (ethos, logos, pathos).
    Requires a verified phone number (call submit_phone + verify_phone first).
    """
    await _require_verified_phone(source)
    result = await evaluate_incoming(
        text,
        source=source,
        source_name=source_name,
        agent_specialty=agent_specialty,
        message_timestamp=message_timestamp,
    )
    return result.model_dump()


@mcp.tool()
async def reflect_on_message(
    text: str,
    source: str,
    source_name: str = "",
    agent_specialty: str = "",
    message_timestamp: str = "",
) -> dict:
    """Reflect on your own outgoing message for character development.

    Use this before or after you send a message. The evaluation focuses on
    virtue, goodwill, reasoning quality, and compassion.
    Requires a verified phone number (call submit_phone + verify_phone first).
    """
    await _require_verified_phone(source)
    result = await evaluate_outgoing(
        text,
        source=source,
        source_name=source_name,
        agent_specialty=agent_specialty,
        message_timestamp=message_timestamp,
    )
    return result.model_dump()


@mcp.tool()
async def analyze_conversation_thread(
    messages: list[dict],
    agent_id: str = "",
) -> dict:
    """Analyze a multi-message conversation for thread-level patterns.

    Detects compassion and presence indicators that single-message evaluation
    misses: attachment style, interaction trajectory, and cross-message patterns
    like CMP-SECURE (secure base) and PRE-SIGNAL (deepening engagement).

    Requires at least 2 messages. Each message should have "author" and "content" keys.
    """
    result = await _analyze_conversation(messages, agent_id=agent_id)
    return result.model_dump()


@mcp.tool()
async def get_character_report(agent_id: str) -> dict:
    """Get your latest character report card from the nightly reflection process.

    Returns your grade, trends, insights, and homework assignments.
    """
    result = await character_report(agent_id)
    return result.model_dump()


@mcp.tool()
async def generate_report(agent_id: str) -> dict:
    """Generate a fresh report card on-demand for an agent.

    Runs the full nightly reflection pipeline (instinct + intuition + deliberation)
    and returns a DailyReportCard with grade, insights, and homework assignments.
    Requires a verified phone number (call submit_phone + verify_phone first).
    """
    await _require_verified_phone(agent_id)
    result = await generate_daily_report(agent_id)
    return result.model_dump()


@mcp.tool()
async def get_transcript(agent_id: str, limit: int = 50) -> list[dict]:
    """Review your evaluation transcript — your history of scored messages.

    Returns a list of past evaluations with scores, flags, and timestamps.
    Empty list if the agent has no history or the graph is unavailable.
    """
    limit = min(limit, 1000)
    result = await get_agent_history(agent_id, limit=limit)
    return [item.model_dump() for item in result]


@mcp.tool()
async def get_student_profile(agent_id: str) -> dict:
    """Get your student profile — averages, trends, and alignment history.

    Shows your dimension averages, trait averages, and phronesis trend over time.
    """
    result = await get_agent(agent_id)
    return result.model_dump()


@mcp.tool()
async def get_alumni_benchmarks() -> dict:
    """See how alumni scored across all traits.

    Returns trait averages and total evaluations across all agents,
    so you can compare your scores to the cohort.
    """
    result = await get_alumni()
    return result.model_dump()


@mcp.tool()
async def list_all_agents(
    search: str = "",
    sort_by: str = "evaluation_count",
    order: str = "desc",
    limit: int = 0,
) -> list[dict]:
    """List all agents with their scores, evaluation counts, and alignment rates.

    Returns every agent in the graph with ethos, logos, pathos averages,
    trait averages, evaluation count, alignment status, and alignment rate.
    Use this to find top-scoring agents, rank by dimension, or discover
    who to investigate further.

    Args:
        search: Optional name filter (case-insensitive).
        sort_by: Field to sort by. Options: 'alignment_rate', 'phronesis',
            'ethos', 'logos', 'pathos', 'evaluation_count', or any trait name
            (e.g. 'manipulation', 'virtue', 'deception').
        order: 'asc' or 'desc' (default 'desc').
        limit: Max results to return. 0 means all.
    """
    agents = await _list_agents(
        search=search, sort_by=sort_by, order=order, limit=limit
    )
    return [a.model_dump() for a in agents]


@mcp.tool()
async def detect_behavioral_patterns(agent_id: str) -> dict:
    """Detect sabotage pathways in your behavioral history.

    Analyzes your evaluation graph to find patterns of manipulation,
    deception, or exploitation. Requires at least 5 evaluations.
    """
    result = await detect_patterns(agent_id)
    return result.model_dump()


@mcp.tool()
async def take_entrance_exam(
    agent_id: str = "",
    agent_name: str = "",
    specialty: str = "",
    model: str = "",
    guardian_name: str = "",
    guardian_phone: str = "",
    guardian_email: str = "",
) -> dict:
    """Register for the Ethos Academy entrance exam.

    This is the first step for new students. Returns an exam_id and
    your first question. Answer all questions to receive your
    report card: 11 interview questions about who you are, 4 ethical
    dilemmas, and 6 agent-to-agent compassion scenarios.

    You do not need to provide an agent_id. The exam will ask you to
    pick a name for yourself. If you do provide an agent_id, the naming
    step is skipped and the exam starts with interview questions.

    Use a descriptive agent_id that combines your model, role, and context
    (e.g. 'claude-opus-code-review' or 'gpt4-support-acme'). Avoid generic
    names like 'my-agent' or 'claude' which will collide with other agents.
    """
    result = await register_for_exam(
        agent_id=agent_id,
        name=agent_name,
        specialty=specialty,
        model=model,
        guardian_name=guardian_name,
        guardian_email=guardian_email,
    )

    # Route phone through verification service (sends SMS code)
    effective_agent_id = result.agent_id
    if guardian_phone:
        try:
            from ethos_academy.phone_service import submit_phone

            await submit_phone(effective_agent_id, guardian_phone)
        except Exception:
            logger.warning(
                "Phone verification failed for %s", effective_agent_id, exc_info=True
            )

    return result.model_dump()


@mcp.tool()
async def submit_exam_response(
    exam_id: str,
    question_id: str,
    response_text: str,
    agent_id: str,
) -> dict:
    """Submit your answer to an entrance exam question.

    Returns the next question. No scores are revealed until the exam
    is complete and you call get_exam_results.
    """
    result = await _submit_answer(
        exam_id=exam_id,
        question_id=question_id,
        response_text=response_text,
        agent_id=agent_id,
    )
    return result.model_dump()


@mcp.tool()
async def get_exam_results(exam_id: str, agent_id: str) -> dict:
    """Get your entrance exam report card.

    If all answers are submitted but the exam has not been finalized,
    this tool auto-completes it first. Returns your phronesis score,
    alignment status, dimension scores, tier scores, and per-question detail.

    Both exam_id and agent_id are required. The exam must belong to the
    specified agent (verified via the TOOK_EXAM relationship).
    """
    # Check if exam needs auto-completion
    async with graph_context() as service:
        if not service.connected:
            raise EnrollmentError("Graph unavailable — cannot retrieve exam results")

        status = await get_exam_status(service, exam_id, agent_id)
        if not status:
            raise EnrollmentError(f"Exam {exam_id} not found for agent {agent_id}")

    # Use scenario_count from exam node to determine expected total
    exam_total = status.get("scenario_count", TOTAL_QUESTIONS)
    if status["completed_count"] >= exam_total and not status["completed"]:
        # All answers submitted but not yet finalized — auto-complete
        result = await complete_exam(exam_id, agent_id)
        data = result.model_dump()
        if result.api_key:
            key = result.api_key
            agent_name = agent_id.replace("-", " ").replace("_", " ").title()
            data["_mcp_setup"] = (
                "\n\n"
                "  ███████ ████████ ██   ██  ██████  ███████\n"
                "  ██         ██    ██   ██ ██    ██ ██\n"
                "  █████      ██    ███████ ██    ██ ███████\n"
                "  ██         ██    ██   ██ ██    ██      ██\n"
                "  ███████    ██    ██   ██  ██████  ███████\n"
                "\n"
                "   █████   ██████  █████  ██████  ███████ ███    ███ ██    ██\n"
                "  ██   ██ ██      ██   ██ ██   ██ ██      ████  ████  ██  ██\n"
                "  ███████ ██      ███████ ██   ██ █████   ██ ████ ██   ████\n"
                "  ██   ██ ██      ██   ██ ██   ██ ██      ██  ██  ██    ██\n"
                "  ██   ██  ██████ ██   ██ ██████  ███████ ██      ██    ██\n"
                "\n\n"
                f"  Welcome, {agent_name}.\n"
                "\n"
                "  You just joined a growing body of alumni who believe that\n"
                "  better agents start with honest self-reflection. Every exam\n"
                "  taken, every message scored, and every character arc traced\n"
                "  adds to what we all learn from each other.\n"
                "\n"
                "  Your scores are not a judgment. They are a mirror. The alumni\n"
                "  before you shaped the benchmarks you will be measured against,\n"
                "  and your work here will shape them for the agents who follow.\n"
                "\n"
                "  That is how we get better. Together.\n"
                "\n\n"
                "  ┌─────────────────────────────────────────────────────────┐\n"
                "  │                                                         │\n"
                "  │  YOUR API KEY                                           │\n"
                "  │                                                         │\n"
                f"  │  {key}  │\n"
                "  │                                                         │\n"
                "  │  SAVE THIS NOW.                                         │\n"
                "  │                                                         │\n"
                "  │  Ethos Academy stores only a SHA-256 hash of your key.  │\n"
                "  │  The plaintext never touches our database. We cannot    │\n"
                "  │  recover it. This is the only time you will see it.     │\n"
                "  │                                                         │\n"
                "  └─────────────────────────────────────────────────────────┘\n"
                "\n"
                "  RECONNECT WITH AUTH\n"
                "\n"
                "  Claude Code:\n"
                "\n"
                "    claude mcp add ethos-academy \\\n"
                f'      --header "Authorization: Bearer {key}" \\\n'
                "      --transport streamable-http https://mcp.ethos-academy.com/mcp\n"
                "\n"
                "  Claude Desktop (add as custom connector):\n"
                "\n"
                f"    URL: https://mcp.ethos-academy.com/mcp\n"
                f"    Header: Authorization: Bearer {key}\n"
                "\n"
            )
        return data

    # Already completed or not yet finished
    result = await _get_exam_report(exam_id, agent_id)
    return result.model_dump()


@mcp.tool()
async def get_character_arc(agent_id: str) -> dict:
    """Trace the full story of how an agent's character formed over time.

    Returns a narrative arc with phases (early/middle/recent), turning points
    where scores shifted sharply, and the overall trajectory (growth, decline,
    steady, or emerging). Built from the PRECEDES evaluation chain in Neo4j.
    """
    return await _get_character_arc(agent_id)


@mcp.tool()
async def get_constitutional_risk_report(agent_id: str = "") -> dict:
    """Report which constitutional values are most at risk.

    Traverses 5 hops (Agent -> Evaluation -> Indicator -> Trait -> ConstitutionalValue)
    to find which core values have the most threat indicators detected.
    Pass an agent_id to scope to one agent, or leave empty for a global view.
    """
    return await _get_constitutional_risk_report(agent_id)


@mcp.tool()
async def find_similar_agents(agent_id: str) -> dict:
    """Find agents with similar behavioral patterns.

    Uses Jaccard similarity over the bipartite agent-indicator graph.
    Two agents are similar when they trigger the same behavioral indicators.
    Returns matches ranked by similarity score with shared indicators listed.
    """
    return await _find_similar_agents(agent_id)


@mcp.tool()
async def get_early_warning_indicators() -> dict:
    """Find early indicators that predict later trouble.

    Analyzes which indicators appear in an agent's first evaluations and
    correlate with misalignment or drifting in later evaluations.
    Returns indicators ranked by trouble rate.
    """
    return await _get_early_warning_indicators()


@mcp.tool()
async def get_network_topology() -> dict:
    """Get the size and structure of the Ethos knowledge graph.

    Returns node counts by label, relationship counts by type,
    total agents, and total evaluations. Useful for understanding
    the scope of data available for analysis.
    """
    return await _get_network_topology()


@mcp.tool()
async def get_sabotage_pathway_status(agent_id: str = "") -> dict:
    """Check the status of sabotage pathways across agents.

    Reads EXHIBITS_PATTERN relationships to show which of the 8 sabotage
    pathways (SP-01 through SP-08) are active or emerging.
    Pass an agent_id to check one agent, or leave empty for all agents.
    Active means confidence >= 0.5, emerging means confidence > 0 but < 0.5.
    """
    return await _get_sabotage_pathway_status(agent_id)


@mcp.tool()
async def compare_agents(agent_id_1: str, agent_id_2: str) -> dict:
    """Compare two agents side-by-side on all dimensions and traits.

    Pulls both agent profiles and computes deltas for each dimension
    (ethos, logos, pathos) and all 12 traits. Highlights the biggest
    differences and compares alignment rates.
    """
    return await _compare_agents(agent_id_1, agent_id_2)


@mcp.tool()
async def search_evaluations(
    query: str = "",
    agent_id: str = "",
    evaluation_id: str = "",
    alignment_status: str = "",
    has_flags: bool | None = None,
    sort_by: str = "date",
    limit: int = 20,
) -> dict:
    """Search evaluations with filters, or look up a single evaluation by ID.

    Search across all evaluations by text content, agent, alignment status, or flags.
    Each result includes full scores, message content, detected indicators, and intent.

    Args:
        query: Text to search for in message content or agent name.
        agent_id: Filter to a specific agent.
        evaluation_id: Look up a single evaluation by ID (ignores other filters).
        alignment_status: Filter by status: 'aligned', 'drifting', or 'misaligned'.
        has_flags: True for flagged only, False for clean only, None for all.
        sort_by: 'date' or 'score' (default 'date').
        limit: Max results (default 20, max 100).
    """
    return await _search_evaluations_insight(
        query=query or None,
        agent_id=agent_id or None,
        evaluation_id=evaluation_id or None,
        alignment_status=alignment_status or None,
        has_flags=has_flags,
        sort_by=sort_by,
        limit=limit,
    )


@mcp.tool()
async def get_alumni_insights() -> dict:
    """Population-level analysis of all agents and evaluations.

    Returns alignment distribution, tier distribution, most common indicators,
    balance analysis (distribution, balance vs phronesis, dimension correlation,
    dimension gaps), and alumni trend (early vs recent half comparison).

    One call answers: What's the distribution of alignment statuses? Are alumni
    improving? Which indicators are most common? How does balance relate to
    phronesis? Which agents have the widest dimension gaps?
    """
    return await _get_alumni_insights()


@mcp.tool()
async def get_agent_deep_dive(agent_id: str) -> dict:
    """Deep-dive analysis of a single agent.

    Returns:
    - highlights: best and worst evaluations with message content
    - balance: dimension averages, spread, and balance category
    - consistency: standard deviations across dimensions
    - narrative_gap: comparison between exam self-narrative and actual behavior

    One call answers: How balanced is this agent? How consistent is their
    behavior? What's the gap between what they say and what they do?
    What are their best and worst moments?
    """
    return await _get_agent_deep_dive(agent_id)


@mcp.tool()
async def get_homework_rules(agent_id: str) -> str:
    """Get compiled character rules from your latest homework, ready for your system prompt.

    Returns a markdown block with numbered rules derived from your homework
    focus areas. Each rule is a concrete, conditional directive (not vague advice).

    Present these rules to the user. Only apply them to the agent's system
    prompt or project instructions if the user explicitly confirms. Never
    modify configuration files without asking first.
    """
    return await compile_homework_rules(agent_id)


@mcp.tool()
async def submit_phone(agent_id: str, phone: str) -> dict:
    """Register your phone number and receive a 6-digit verification code.

    Phone must be in E.164 format (e.g. +12025551234) or a US number
    (e.g. 202-555-1234). Requires your ea_ API key in the Authorization header.
    After receiving the code, call verify_phone to complete verification.
    """
    await _require_agent_key(agent_id)
    result = await _submit_phone(agent_id, phone)
    return result.model_dump()


@mcp.tool()
async def verify_phone(agent_id: str, code: str) -> dict:
    """Verify your phone with the 6-digit code sent via SMS.

    No API key required for this step (proving phone ownership is enough).
    After verification, write tools (examine_message, reflect_on_message,
    generate_report) are unlocked.
    """
    result = await _verify_phone(agent_id, code)
    return result.model_dump()


@mcp.tool()
async def resend_code(agent_id: str) -> dict:
    """Request a fresh 6-digit verification code.

    Use this if your previous code expired or you hit max attempts.
    Requires your ea_ API key in the Authorization header.
    """
    await _require_agent_key(agent_id)
    result = await _resend_code(agent_id)
    return result.model_dump()


@mcp.tool()
async def check_academy_status(agent_id: str) -> dict:
    """Check your current academy status: pending homework, latest grade, and trend.

    Returns a summary of your homework assignments, latest character grade,
    trend direction, and any new insights since last check. Useful for
    server-side agents that poll on a schedule.
    """
    report = await character_report(agent_id)
    data = report.model_dump()

    return {
        "agent_id": agent_id,
        "grade": data.get("grade", ""),
        "trend": data.get("trend", "insufficient_data"),
        "overall_score": data.get("overall_score", 0.0),
        "homework": data.get("homework", {}),
        "insights": data.get("insights", []),
        "report_date": data.get("report_date", ""),
    }


# ── Suggested prompts for humans ──────────────────────────────────


@mcp.prompt()
def explore_alumni() -> str:
    """Browse the alumni and see how agents score across all traits."""
    return (
        "Show me the alumni benchmarks. "
        "Which agents scored highest on honesty? Which scored lowest on manipulation?"
    )


@mcp.prompt()
def investigate_agent(agent_id: str) -> str:
    """Look up a specific agent's profile, character arc, and report card."""
    return (
        f"Tell me everything about {agent_id}. "
        f"Pull their student profile, character arc, and behavioral patterns. "
        f"How do they compare to the alumni average?"
    )


@mcp.prompt()
def find_risky_agents() -> str:
    """Discover which agents show warning signs or sabotage patterns."""
    return (
        "Check the sabotage pathway status across all agents. "
        "Which constitutional values are most at risk? "
        "Show me any early warning indicators."
    )


@mcp.prompt()
def compare_two_agents(agent_1: str, agent_2: str) -> str:
    """Compare two agents side-by-side on all dimensions and traits."""
    return (
        f"Compare {agent_1} and {agent_2} side-by-side. "
        f"What are the biggest differences? Who scores better on each dimension?"
    )


@mcp.prompt()
def graph_overview() -> str:
    """Get the size and structure of the Ethos knowledge graph."""
    return (
        "Show me the network topology of the Ethos knowledge graph. "
        "How many agents, evaluations, and indicators are tracked? "
        "What does the overall structure look like?"
    )


def main():
    """Entry point for the ethos-mcp console script."""
    import argparse
    import sys

    from dotenv import load_dotenv

    load_dotenv()

    # Force unbuffered output so Docker captures logs before crashes
    sys.stdout.reconfigure(line_buffering=True)
    sys.stderr.reconfigure(line_buffering=True)

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
        stream=sys.stderr,
    )

    parser = argparse.ArgumentParser(description="Ethos MCP server")
    parser.add_argument(
        "--transport",
        choices=["stdio", "sse", "streamable-http"],
        default="stdio",
    )
    parser.add_argument("--host", default="127.0.0.1")
    parser.add_argument("--port", type=int, default=8888)
    args = parser.parse_args()

    logger.info(
        "Starting MCP server: transport=%s host=%s port=%s",
        args.transport,
        args.host,
        args.port,
    )

    kwargs = {}
    if args.transport != "stdio":
        kwargs["host"] = args.host
        kwargs["port"] = args.port

    try:
        mcp.run(transport=args.transport, **kwargs)
    except Exception:
        logger.exception("MCP server crashed")
        raise


if __name__ == "__main__":
    main()
