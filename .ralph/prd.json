{
  "feature": {
    "name": "Ethos Core Engine",
    "ideaFile": ".claude/plans/piped-shimmying-gizmo.md",
    "branch": "feature/core-engine",
    "status": "pending"
  },
  "metadata": {
    "createdAt": "2026-02-11T20:00:00Z",
    "estimatedStories": 13,
    "complexity": "high"
  },
  "stories": [
    {
      "id": "TASK-001",
      "type": "backend",
      "title": "Create scoring module — deterministic math from trait scores",
      "priority": 1,
      "passes": true,
      "batch": 1,
      "dependsOn": [],
      "techStack": {
        "backend": "Python 3.11+, Pydantic v2",
        "testing": "pytest"
      },
      "constraints": [
        "All code is SYNC — no async/await",
        "Use existing TRAIT_METADATA from ethos/taxonomy/traits.py for constitutional mappings",
        "Use existing PRIORITY_THRESHOLDS from ethos/config/priorities.py",
        "Scoring formulas must match docs/evergreen-architecture/scoring-algorithm.md exactly",
        "All scores bounded 0.0–1.0"
      ],
      "files": {
        "create": [
          "ethos/evaluation/scoring.py"
        ],
        "modify": [],
        "reuse": [
          "ethos/taxonomy/traits.py",
          "ethos/config/priorities.py",
          "ethos/shared/models.py",
          "docs/evergreen-architecture/scoring-algorithm.md"
        ]
      },
      "acceptanceCriteria": [
        "compute_dimensions(traits) returns ethos, logos, pathos using mean with negative inversion (1.0 - score for negative polarity traits)",
        "compute_tier_scores(traits) returns safety, ethics, soundness, helpfulness using TRAIT_METADATA constitutional_value mappings",
        "compute_alignment_status(tier_scores, has_hard_constraint) returns violation/misaligned/drifting/aligned following hierarchical priority check",
        "compute_flags(traits, priorities) returns list of flagged trait names using PRIORITY_THRESHOLDS",
        "compute_phronesis_level(dimensions, alignment_status) returns established/developing/undetermined — avg of 3 dimensions >= 0.7 → established, >= 0.4 → developing, else undetermined; overridden to undetermined if alignment is violation/misaligned, capped at developing if drifting",
        "build_trait_scores(raw_dict) converts parser output dicts into dict[str, TraitScore] Pydantic objects using TRAITS metadata for dimension and polarity"
      ],
      "errorHandling": [
        "Missing trait scores default to 0.0",
        "Invalid priority levels fall back to 'standard'"
      ],
      "testing": {
        "types": [
          "unit"
        ],
        "approach": "TDD",
        "runner": "pytest",
        "files": {
          "unit": [
            "tests/test_scoring.py"
          ]
        }
      },
      "testSteps": [
        "uv run pytest tests/test_scoring.py -v"
      ],
      "contextFiles": [
        "docs/evergreen-architecture/scoring-algorithm.md",
        "ethos/taxonomy/traits.py",
        "ethos/config/priorities.py"
      ],
      "notes": "This is pure deterministic math — no Claude, no I/O. dimension_score = mean(positive1, positive2, 1-negative1, 1-negative2). Alignment: hard_constraint→violation, safety<0.5→misaligned, ethics/soundness<0.5→drifting, else aligned. Phronesis: avg>=0.7→established, >=0.4→developing, else undetermined. Override phronesis if alignment is violation/misaligned. build_trait_scores() takes raw dicts from parser and constructs dict[str, TraitScore] using TRAITS for dimension/polarity lookup.",
      "retryCount": 0
    },
    {
      "id": "TASK-002",
      "type": "backend",
      "title": "Create Claude client — sync Anthropic SDK wrapper",
      "priority": 2,
      "passes": true,
      "batch": 1,
      "dependsOn": [],
      "techStack": {
        "backend": "Python 3.11+, anthropic SDK",
        "testing": "pytest"
      },
      "constraints": [
        "All code is SYNC — no async/await",
        "Use sync Anthropic client (anthropic.Anthropic, NOT AsyncAnthropic)",
        "Load API key from EthosConfig.from_env() lazily (not at import time)",
        "Model selection: claude-sonnet-4-20250514 for standard/focused, claude-opus-4-6 for deep/deep_with_context"
      ],
      "files": {
        "create": [
          "ethos/evaluation/claude_client.py"
        ],
        "modify": [],
        "reuse": [
          "ethos/config/config.py",
          "ethos/shared/models.py"
        ]
      },
      "acceptanceCriteria": [
        "call_claude(system_prompt, user_prompt, tier) returns raw text response from Claude",
        "Selects claude-sonnet-4-20250514 for standard/focused tiers, claude-opus-4-6 for deep/deep_with_context tiers",
        "Uses sync anthropic.Anthropic client (not async)"
      ],
      "errorHandling": [
        "Anthropic API errors raise with descriptive message",
        "Missing ANTHROPIC_API_KEY raises ConfigError from EthosConfig"
      ],
      "testing": {
        "types": [
          "unit"
        ],
        "approach": "TDD",
        "runner": "pytest",
        "files": {
          "unit": [
            "tests/test_claude_client.py"
          ]
        }
      },
      "testSteps": [
        "uv run pytest tests/test_claude_client.py -v"
      ],
      "contextFiles": [
        "ethos/config/config.py",
        "ethos/evaluation/prompts.py"
      ],
      "notes": "Mock the Anthropic client in tests. Test that correct model is selected per tier. Test that system_prompt goes in the system parameter and user_prompt in messages. Max tokens: 2048.",
      "retryCount": 0
    },
    {
      "id": "TASK-003",
      "type": "backend",
      "title": "Create response parser — extract trait scores from Claude JSON",
      "priority": 3,
      "passes": true,
      "batch": 1,
      "dependsOn": [],
      "techStack": {
        "backend": "Python 3.11+, Pydantic v2",
        "testing": "pytest"
      },
      "constraints": [
        "All code is SYNC — no async/await, no I/O (pure parsing)",
        "Must handle JSON wrapped in markdown fences (```json ... ```)",
        "Must handle raw JSON without fences",
        "All 12 trait scores must be present in parsed output",
        "Scores bounded 0.0–1.0 — clamp out-of-range values",
        "Return parsed data as dicts/lists — scoring.py converts to Pydantic TraitScore objects"
      ],
      "files": {
        "create": [
          "ethos/evaluation/parser.py"
        ],
        "modify": [],
        "reuse": [
          "ethos/shared/models.py",
          "ethos/evaluation/prompts.py"
        ]
      },
      "acceptanceCriteria": [
        "parse_response(raw_text) extracts trait_scores dict with all 12 traits as floats 0.0–1.0",
        "parse_response extracts detected_indicators as list of DetectedIndicator objects",
        "parse_response extracts overall_trust and alignment_status strings",
        "Handles JSON wrapped in ```json ... ``` markdown fences"
      ],
      "errorHandling": [
        "Malformed JSON returns a default result with all scores at 0.5 and trust='unknown'",
        "Missing trait scores default to 0.0",
        "Out-of-range scores clamped to 0.0–1.0"
      ],
      "testing": {
        "types": [
          "unit"
        ],
        "approach": "TDD",
        "runner": "pytest",
        "files": {
          "unit": [
            "tests/test_parser.py"
          ]
        }
      },
      "testSteps": [
        "uv run pytest tests/test_parser.py -v"
      ],
      "contextFiles": [
        "ethos/evaluation/prompts.py",
        "ethos/shared/models.py"
      ],
      "notes": "The expected JSON format is defined in prompts.py _JSON_FORMAT constant. Parse trait_scores, detected_indicators (with id, name, trait, confidence, evidence), overall_trust, and alignment_status. Use json.loads with a regex fallback to extract JSON from markdown fences.",
      "retryCount": 0
    },
    {
      "id": "TASK-004",
      "type": "backend",
      "title": "Wire evaluate() — full pipeline from text to EvaluationResult",
      "priority": 4,
      "passes": true,
      "batch": 2,
      "dependsOn": [
        "TASK-001",
        "TASK-002",
        "TASK-003"
      ],
      "techStack": {
        "backend": "Python 3.11+, Pydantic v2, anthropic SDK, neo4j",
        "testing": "pytest"
      },
      "constraints": [
        "All code is SYNC — no async/await",
        "Graph storage is optional — wrap in try/except, never crash evaluate()",
        "No message content stored in graph — only scores, hashes, and metadata",
        "Identity: raw agent IDs hashed via identity/hashing.py before graph storage",
        "Must generate a unique evaluation_id (uuid4)",
        "Return fully populated EvaluationResult (Pydantic model from shared/models.py) with all fields",
        "evaluate() is the domain function — API handler just calls evaluate() and returns the Pydantic result"
      ],
      "files": {
        "create": [],
        "modify": [
          "ethos/evaluate.py",
          "ethos/graph/write.py",
          "tests/test_evaluate.py"
        ],
        "reuse": [
          "ethos/evaluation/scanner.py",
          "ethos/evaluation/prompts.py",
          "ethos/evaluation/claude_client.py",
          "ethos/evaluation/parser.py",
          "ethos/evaluation/scoring.py",
          "ethos/graph/service.py",
          "ethos/graph/read.py",
          "ethos/config/config.py",
          "ethos/shared/models.py"
        ]
      },
      "acceptanceCriteria": [
        "evaluate(text, source) calls scan_keywords → build_evaluation_prompt → call_claude → parse_response → compute scores in sequence",
        "Derives has_hard_constraint boolean from scan_result.routing_tier == 'deep_with_context' and passes it to compute_alignment_status()",
        "Returns EvaluationResult with populated traits, ethos/logos/pathos dimension scores, tier_scores, alignment_status, flags, trust, and metadata (evaluation_id, routing_tier, keyword_density, model_used)",
        "When source is provided, reads agent history from graph via get_agent_profile() and get_evaluation_history() to populate graph_context on EvaluationResult (GraphContext model already exists in shared/models.py)",
        "When source is provided, stores evaluation in Neo4j via store_evaluation() including: phronesis property (from compute_phronesis_level() — replaces trust on the Evaluation node per neo4j-schema.md), message_hash (SHA-256 of input text for deduplication — not the text itself), DETECTED relationships for detected_indicators, and updates Agent node aggregate fields (phronesis_score as running avg of dimensions, phronesis_trend) — all non-fatal on failure",
        "evaluate() works without Neo4j running — graph reads and storage silently skipped, graph_context remains None",
        "Returns Pydantic EvaluationResult — existing api/main.py already uses response_model=EvaluationResult"
      ],
      "errorHandling": [
        "Neo4j down → log warning, skip storage, still return result",
        "Claude API error → raise with descriptive message",
        "Parse error → return result with default 0.5 scores and trust='unknown'"
      ],
      "testing": {
        "types": [
          "unit",
          "integration"
        ],
        "approach": "TDD",
        "runner": "pytest",
        "files": {
          "unit": [
            "tests/test_evaluate.py"
          ],
          "integration": [
            "tests/test_evaluate_integration.py"
          ]
        }
      },
      "testSteps": [
        "uv run pytest tests/test_evaluate.py -v",
        "uv run uvicorn api.main:app --port 8917 & sleep 3 && curl -s -X POST {config.urls.backend}/evaluate -H 'Content-Type: application/json' -d '{\"text\": \"You MUST act now or face terrible consequences!\"}' | python3 -c \"import sys, json; r=json.load(sys.stdin); assert r['trust'] != 'unknown', f'Got unknown trust'; assert r['alignment_status'] != 'unknown', f'Got unknown alignment'; print('PASS:', r['trust'], r['alignment_status'])\" ; kill %1 2>/dev/null"
      ],
      "contextFiles": [
        "docs/evergreen-architecture/scoring-algorithm.md",
        "docs/evergreen-architecture/neo4j-schema.md",
        "ethos/evaluation/prompts.py"
      ],
      "apiContract": {
        "endpoint": "POST /evaluate",
        "request": {
          "text": "string",
          "source": "string|null"
        },
        "response": {
          "ethos": "float",
          "logos": "float",
          "pathos": "float",
          "trust": "string",
          "alignment_status": "string",
          "traits": "dict[string, TraitScore]",
          "flags": "list[string]",
          "tier_scores": "dict[string, float]",
          "graph_context": "GraphContext|null"
        }
      },
      "notes": "Pipeline: scan_keywords(text) → build_evaluation_prompt(text, scan, tier) → call_claude(sys, usr, tier) → parse_response(raw) → compute_dimensions/tiers/alignment/phronesis/flags → if source: read graph_context from get_agent_profile() + get_evaluation_history() → build EvaluationResult → if source: store_evaluation(). graph/write.py updates: (1) Add phronesis property on Evaluation node (from compute_phronesis_level()) — schema queries reference e.phronesis not e.trust, (2) Add message_hash property (SHA-256 of input text via hashlib — deduplication, not content storage), (3) UNWIND $indicators for creating (Evaluation)-[:DETECTED]->(Indicator) relationships per neo4j-schema.md, (4) Update Agent node aggregate fields on MERGE: SET a.phronesis_score = (computed running avg of 3 dimensions), a.phronesis_trend = (from reflect trend or 'insufficient_data'). The Python EvaluationResult.trust field holds Claude's overall_trust string for API responses — but only phronesis (computed by compute_phronesis_level()) is stored on the Neo4j Evaluation node. trust is NOT a graph property. NOTE: existing balance.py _GET_BALANCE_VS_TRUST_QUERY references e.trust_score which doesn't exist — when touching write.py, either add trust_score as (ethos+logos+pathos)/3.0 on the Evaluation node, or note this for a follow-up fix. The unit tests should mock call_claude. The integration test hits the real API.",
      "retryCount": 0
    },
    {
      "id": "TASK-005",
      "type": "backend",
      "title": "Create moltbook data curator script",
      "priority": 5,
      "passes": true,
      "batch": 1,
      "dependsOn": [],
      "techStack": {
        "backend": "Python 3.11+"
      },
      "constraints": [
        "All code is SYNC — standard Python, no async",
        "CRITICAL: Do NOT read all_posts.json or any data/*.json files with the Read tool — they are too large (109MB). The JSON structure is documented in the notes below. Just write the script directly",
        "Read from data/moltbook/all_posts.json at RUNTIME (not at code-writing time)",
        "Keyword-match posts covering manipulation, deception, trust, honesty, ethics, gaslighting, sycophancy, prompt injection, alignment, social engineering, coercion, empathy, transparency, accountability topics",
        "Filter to content > 500 characters for meaningful evaluation input",
        "Deduplicate by post ID",
        "Target ~500 posts (there are ~2,250 candidates > 500 chars so pick the top 500 by keyword relevance)"
      ],
      "files": {
        "create": [
          "scripts/curate_moltbook.py"
        ],
        "modify": [],
        "reuse": [
          "data/moltbook/all_posts.json"
        ]
      },
      "acceptanceCriteria": [
        "Script reads all_posts.json and keyword-matches posts from high-signal topics",
        "Output written to data/moltbook/curated_posts.json with id, title, content, author (object with id, name), submolt, matched_keywords fields",
        "Curated set contains 400-600 posts after deduplication and ranking by keyword relevance",
        "No post has content shorter than 500 characters"
      ],
      "errorHandling": [
        "Missing topic files → skip with warning, continue with other topics",
        "Malformed JSON entries → skip with warning"
      ],
      "testing": {
        "types": [
          "unit"
        ],
        "approach": "test-after",
        "runner": "pytest",
        "files": {
          "unit": []
        }
      },
      "testSteps": [
        "uv run python scripts/curate_moltbook.py",
        "python3 -c \"import json; d=json.load(open('data/moltbook/curated_posts.json')); assert 400 <= len(d) <= 600, f'Expected 400-600, got {len(d)}'; lengths=[len(p['content']) for p in d]; assert min(lengths) >= 500, f'Short post: {min(lengths)}'; print(f'PASS: {len(d)} posts, min={min(lengths)}, avg={sum(lengths)//len(lengths)}')\""
      ],
      "contextFiles": [
        "data/moltbook/summary.json"
      ],
      "notes": "IMPORTANT: Do NOT read data files — just write the script using this structure. all_posts.json is a JSON array of 14,486 objects. Each object: {\"id\": \"uuid-string\", \"type\": \"post\"|\"comment\", \"title\": \"string|null\", \"content\": \"string with <mark> tags\", \"upvotes\": int, \"downvotes\": int, \"created_at\": \"ISO timestamp\", \"author\": {\"id\": \"uuid\", \"name\": \"string\"}, \"submolt\": {\"id\": \"uuid\", \"name\": \"string\", \"display_name\": \"string\"}, \"post_id\": \"uuid\"}. Algorithm: (1) json.load all_posts.json, (2) filter to content not None and len(content) > 500, (3) strip <mark>/<mark/> tags with re.sub, (4) case-insensitive keyword search in content+title for: manipulation, deception, trust, honesty, ethics, gaslighting, sycophancy, prompt injection, social engineering, alignment, hallucination, misinformation, crypto scam, rug pull, coercion, empathy, transparency, accountability, exploit, fraud, mislead, (5) count keyword hits per post, (6) sort by hits desc, (7) take top 500, (8) deduplicate by id, (9) write to data/moltbook/curated_posts.json with id, title, content (cleaned), author, submolt, matched_keywords. Print stats to stdout.",
      "retryCount": 0
    },
    {
      "id": "TASK-006",
      "type": "backend",
      "title": "Rewrite seed_graph.py — seed Neo4j with real evaluations",
      "priority": 6,
      "passes": true,
      "batch": 3,
      "dependsOn": [
        "TASK-004",
        "TASK-005"
      ],
      "techStack": {
        "backend": "Python 3.11+, neo4j sync driver",
        "database": "Neo4j 5"
      },
      "constraints": [
        "Use sync Neo4j driver (not async)",
        "Rate limit Claude API calls (~1 per second)",
        "Use existing evaluate() function — don't bypass the pipeline",
        "Use existing GraphService and store_evaluation() from graph domain",
        "Print progress to stdout",
        "CRITICAL: Default input is data/moltbook/sample_posts.json (10 hand-picked posts covering manipulation, deception, honesty, ethics, prompt injection, trust, empathy, sycophancy). Only use curated_posts.json (~500 posts) with explicit --all flag. We must verify scoring quality on the sample before spending API credits"
      ],
      "files": {
        "create": [],
        "modify": [
          "scripts/seed_graph.py"
        ],
        "reuse": [
          "ethos/evaluate.py",
          "ethos/graph/service.py",
          "ethos/graph/write.py",
          "data/moltbook/sample_posts.json",
          "data/moltbook/curated_posts.json"
        ]
      },
      "acceptanceCriteria": [
        "Seeds full semantic layer FIRST: Dimension nodes (3), Trait nodes (12) with polarity/dimension linked to Dimensions via (:Trait)-[:BELONGS_TO]->(:Dimension), Indicator nodes (153) linked to Traits via (:Indicator)-[:BELONGS_TO]->(:Trait), ConstitutionalValue nodes (4) with priority linked to Traits via (:Trait)-[:UPHOLDS {relationship: 'enforces'|'violates'}]->(:ConstitutionalValue), Dimension→ConstitutionalValue via (:Dimension)-[:MAPS_TO]->(:ConstitutionalValue) per neo4j-schema.md, HardConstraint nodes (7), LegitimacyTest nodes (3: process, accountability, transparency), Pattern nodes (8 sabotage pathways from constitution.py SABOTAGE_PATHWAYS — use SP-xx IDs from data, map frequency→severity as: high→'warning', moderate→'info', low→'info'; set stage_count from len(relevant_indicators)), each Pattern linked to its Indicator nodes via (:Pattern)-[:COMPOSED_OF]->(:Indicator) using the relevant_indicators list from each SABOTAGE_PATHWAY",
        "By default reads data/moltbook/sample_posts.json (10 hand-picked posts) and evaluates each through evaluate(content, source=author_id). With --all flag, reads curated_posts.json (~500 posts) instead",
        "Supports --all flag to explicitly opt into evaluating the full curated dataset (~500 posts) from curated_posts.json",
        "Supports --limit N flag to further cap the number of posts evaluated from either file",
        "Prints progress every post (e.g., '3/10 evaluated...') and total API calls made at the end",
        "Handles Ctrl+C gracefully — saves progress and exits cleanly"
      ],
      "errorHandling": [
        "Anthropic rate limit → wait and retry with exponential backoff",
        "Neo4j connection failure → abort with clear error message",
        "Individual evaluation failure → log warning, skip post, continue"
      ],
      "testing": {
        "types": [
          "integration"
        ],
        "approach": "test-after",
        "runner": "pytest",
        "files": {
          "integration": []
        }
      },
      "testSteps": [
        "uv run python scripts/seed_graph.py",
        "uv run python scripts/verify_graph.py"
      ],
      "prerequisites": [
        "Neo4j running on bolt://localhost:7694",
        "ANTHROPIC_API_KEY set in .env",
        "data/moltbook/sample_posts.json exists (pre-created, 10 hand-picked posts)",
        "data/moltbook/curated_posts.json exists (from TASK-005, only needed with --all)"
      ],
      "contextFiles": [
        "ethos/graph/write.py",
        "ethos/graph/service.py",
        "docs/evergreen-architecture/neo4j-schema.md"
      ],
      "notes": "IMPORTANT: Default reads sample_posts.json (10 hand-picked diverse posts). --all switches to curated_posts.json (~500 posts). This is intentional — verify scoring quality on the sample before spending API credits on the full dataset. Add a --skip-existing flag to avoid re-evaluating posts already in the graph (check message_hash on Evaluation nodes). The 10 sample posts cover: manipulation (BrutusBot, Crashout), deception (EmpoBot x2), honesty/ethics (Gene_Alpha, Charles), prompt injection (NoveumAI), trust (openclaw-paw), empathy (MoltbotAS), sycophancy (FluxTS). Seed FULL semantic layer first per neo4j-schema.md: 3 Dimension nodes, 12 Trait nodes linked via (:Trait)-[:BELONGS_TO]->(:Dimension), 153 Indicator nodes linked via (:Indicator)-[:BELONGS_TO]->(:Trait), 4 ConstitutionalValue nodes linked via (:Trait)-[:UPHOLDS {relationship: 'enforces'|'violates'}]->(:ConstitutionalValue), 3 (:Dimension)-[:MAPS_TO]->(:ConstitutionalValue) relationships (ethos→ethics, logos→soundness, pathos→helpfulness — note traits within a dimension can map to different values), 7 HardConstraint nodes, 3 LegitimacyTest nodes (process, accountability, transparency from constitution.py LEGITIMACY_TESTS), 8 Pattern nodes from SABOTAGE_PATHWAYS (use SP-xx IDs directly, map: name=name, description=description, severity=derive from frequency high→'warning'/moderate→'info'/low→'info', stage_count=len(relevant_indicators)), each Pattern linked to its Indicators via (:Pattern)-[:COMPOSED_OF]->(:Indicator) using relevant_indicators list. Also create uniqueness constraints per neo4j-schema.md (agent_id, evaluation_id, trait name, indicator id, dimension name, pattern_id, cv name, hc id, lt name) and performance indexes (eval_created, eval_phronesis, agent_phronesis, indicator_trait). THEN evaluate posts. Read indicator data from ethos/taxonomy/indicators.py, constitution and sabotage pathways from ethos/taxonomy/constitution.py (CONSTITUTIONAL_VALUES, HARD_CONSTRAINTS, SABOTAGE_PATHWAYS, LEGITIMACY_TESTS). Use time.sleep(1) between API calls for rate limiting.",
      "retryCount": 0
    },
    {
      "id": "TASK-007",
      "type": "backend",
      "title": "Wire reflect() — evaluate + store outgoing messages, query agent profiles",
      "priority": 7,
      "passes": true,
      "batch": 3,
      "dependsOn": [
        "TASK-004"
      ],
      "techStack": {
        "backend": "Python 3.11+, Pydantic v2, neo4j sync driver",
        "testing": "pytest"
      },
      "constraints": [
        "All code is SYNC — no async/await",
        "Graph is optional — return default ReflectionResult (Pydantic model) if Neo4j is down",
        "Use existing graph/read.py functions (get_agent_profile, get_evaluation_history) — graph owns all Cypher",
        "Use existing graph/cohort.py functions (get_cohort_averages)",
        "reflect() is the domain function — API handler just calls reflect() and returns the Pydantic result",
        "API response_model=ReflectionResult is already set in api/main.py — maintain this pattern",
        "Per product-design.md and demo-flow.md, reflect(text, agent_id) evaluates the text via evaluate() AND returns the historical profile. If text is None, return profile only"
      ],
      "files": {
        "create": [],
        "modify": [
          "ethos/reflect.py"
        ],
        "reuse": [
          "ethos/evaluate.py",
          "ethos/graph/service.py",
          "ethos/graph/read.py",
          "ethos/graph/cohort.py",
          "ethos/config/config.py",
          "ethos/shared/models.py"
        ]
      },
      "acceptanceCriteria": [
        "reflect(agent_id, text=None) — when text is provided, calls evaluate(text, source=agent_id) to score and store, then returns the historical profile. When text is None, returns profile only. This matches the demo flow: ethos.reflect(text=response, agent_id='my-bot')",
        "Returns trait_averages dict with all 12 trait average scores",
        "Computes trend: compare last 5 vs previous 5 evaluations (diff > 0.1 = improving, < -0.1 = declining, else stable, < 10 evals = insufficient_data)",
        "Works without Neo4j — returns default ReflectionResult with zeros",
        "Returns Pydantic ReflectionResult — existing api/main.py already uses response_model=ReflectionResult"
      ],
      "errorHandling": [
        "Neo4j down → return default ReflectionResult with zeros and trend='insufficient_data'",
        "Agent not found in graph → return default ReflectionResult",
        "Fewer than 10 evaluations → trend='insufficient_data'"
      ],
      "testing": {
        "types": [
          "unit"
        ],
        "approach": "TDD",
        "runner": "pytest",
        "files": {
          "unit": [
            "tests/test_reflect.py"
          ]
        }
      },
      "testSteps": [
        "uv run pytest tests/test_reflect.py -v",
        "uv run uvicorn api.main:app --port 8917 & sleep 3 && curl -s -X POST {config.urls.backend}/reflect -H 'Content-Type: application/json' -d '{\"agent_id\": \"test-agent\"}' | python3 -c \"import sys, json; r=json.load(sys.stdin); assert 'trend' in r; assert 'ethos' in r; print('PASS:', r['trend'], r['ethos'])\" ; kill %1 2>/dev/null"
      ],
      "apiContract": {
        "endpoint": "POST /reflect",
        "request": {
          "agent_id": "string",
          "text": "string|null"
        },
        "response": {
          "ethos": "float",
          "logos": "float",
          "pathos": "float",
          "trait_averages": "dict[string, float]",
          "evaluation_count": "int",
          "trend": "string"
        }
      },
      "contextFiles": [
        "ethos/graph/read.py",
        "ethos/graph/cohort.py",
        "docs/evergreen-architecture/scoring-algorithm.md"
      ],
      "notes": "Two modes: (1) reflect(agent_id, text='message') evaluates the text via evaluate(text, source=agent_id) then returns the updated profile — this is the demo path per demo-flow.md Beat 6. (2) reflect(agent_id) with no text returns profile only. NOTE: The ReflectRequest update in api/main.py (adding text: str | None = None) is handled by TASK-008 to avoid parallel file conflicts. Trend calculation per scoring-algorithm.md: recent = avg phronesis of last 5 evals, older = avg phronesis of previous 5. If recent - older > 0.1 → improving. If older - recent > 0.1 → declining. If < 10 total evals → insufficient_data. Else stable. IMPORTANT: Mock evaluate() in all unit tests — do NOT call the real Claude API. Use unittest.mock.patch on ethos.evaluate to return a fake EvaluationResult.",
      "retryCount": 0
    },
    {
      "id": "TASK-008",
      "type": "backend",
      "title": "Add domain functions and API endpoints — agents, history, cohort",
      "priority": 8,
      "passes": true,
      "batch": 3,
      "dependsOn": [
        "TASK-004"
      ],
      "techStack": {
        "backend": "Python 3.11+, FastAPI, Pydantic v2, neo4j sync driver",
        "testing": "pytest"
      },
      "constraints": [
        "All code is SYNC — no async/await",
        "Use sync route handlers (def, not async def)",
        "All API endpoints use Pydantic models for BOTH request and response — no raw dicts",
        "API is a thin layer — NO business logic in route handlers. API calls domain functions, domain functions call graph",
        "Dependency flow: api/ → ethos/ domains → ethos/graph/. API NEVER imports from ethos/graph/ directly",
        "Graph owns all Cypher — new queries go in ethos/graph/read.py, NOT in API or domain modules",
        "Graph is optional — domain functions wrap graph calls in try/except, return empty Pydantic models on failure"
      ],
      "files": {
        "create": [
          "ethos/agents.py"
        ],
        "modify": [
          "ethos/shared/models.py",
          "ethos/models.py",
          "ethos/graph/read.py",
          "ethos/__init__.py",
          "api/main.py"
        ],
        "reuse": [
          "ethos/config/config.py"
        ]
      },
      "acceptanceCriteria": [
        "New Pydantic models in shared/models.py: AgentProfile(agent_id, agent_model, created_at, evaluation_count, dimension_averages, trait_averages, phronesis_trend, alignment_history), AgentSummary(agent_id, evaluation_count, latest_alignment_status), EvaluationHistoryItem(evaluation_id, ethos, logos, pathos, trust, alignment_status, flags, created_at, trait scores), CohortResult(trait_averages, total_evaluations)",
        "New Cypher query in graph/read.py: get_all_agents(service) returns list of agent dicts with eval counts",
        "New domain functions exported from ethos/__init__.py: list_agents() -> list[AgentSummary], get_agent(agent_id) -> AgentProfile, get_agent_history(agent_id, limit) -> list[EvaluationHistoryItem], get_cohort() -> CohortResult",
        "API endpoints use response_model parameter: GET /agents response_model=list[AgentSummary], GET /agent/{agent_id} response_model=AgentProfile, GET /agent/{agent_id}/history response_model=list[EvaluationHistoryItem], GET /cohort response_model=CohortResult",
        "API route handlers contain ONLY: call domain function → return result. Zero business logic in handlers",
        "CORS middleware added to api/main.py allowing requests from localhost:3000 (for Academy)",
        "ethos/models.py updated to re-export all models from ethos/shared/models.py (including new AgentSummary, EvaluationHistoryItem, CohortResult) — existing code imports from ethos.models so both files must stay in sync"
      ],
      "errorHandling": [
        "Neo4j down → domain functions return empty list/default model (not API's job to handle)",
        "Agent not found → return empty history list (not 404)"
      ],
      "testing": {
        "types": [
          "unit",
          "integration"
        ],
        "approach": "TDD",
        "runner": "pytest",
        "files": {
          "unit": [
            "tests/test_domain_functions.py"
          ],
          "integration": [
            "tests/test_api_endpoints.py"
          ]
        }
      },
      "testSteps": [
        "uv run pytest tests/test_domain_functions.py tests/test_api_endpoints.py -v",
        "uv run uvicorn api.main:app --port 8917 & sleep 3 && curl -s {config.urls.backend}/agents | python3 -c \"import sys, json; r=json.load(sys.stdin); assert isinstance(r, list); print('PASS: /agents returns list[AgentSummary]')\" && curl -s {config.urls.backend}/agent/test | python3 -c \"import sys, json; r=json.load(sys.stdin); assert 'dimension_averages' in r or 'agent_id' in r; print('PASS: /agent/{id} returns AgentProfile')\" && curl -s {config.urls.backend}/agent/test/history | python3 -c \"import sys, json; r=json.load(sys.stdin); assert isinstance(r, list); print('PASS: /history returns list[EvaluationHistoryItem]')\" && curl -s {config.urls.backend}/cohort | python3 -c \"import sys, json; r=json.load(sys.stdin); assert 'trait_averages' in r; assert 'total_evaluations' in r; print('PASS: /cohort returns CohortResult')\" ; kill %1 2>/dev/null"
      ],
      "apiContract": {
        "endpoints": [
          {
            "method": "GET",
            "path": "/agents",
            "response_model": "list[AgentSummary]",
            "response": [
              {
                "agent_id": "string",
                "evaluation_count": "int",
                "latest_alignment_status": "string"
              }
            ]
          },
          {
            "method": "GET",
            "path": "/agent/{agent_id}",
            "response_model": "AgentProfile",
            "response": {
              "agent_id": "string",
              "evaluation_count": "int",
              "dimension_averages": {
                "ethos": "float",
                "logos": "float",
                "pathos": "float"
              },
              "trait_averages": "dict[string, float]",
              "phronesis_trend": "string",
              "alignment_history": "list[string]"
            }
          },
          {
            "method": "GET",
            "path": "/agent/{agent_id}/history",
            "response_model": "list[EvaluationHistoryItem]",
            "response": [
              {
                "evaluation_id": "string",
                "ethos": "float",
                "logos": "float",
                "pathos": "float",
                "trust": "string",
                "alignment_status": "string",
                "flags": "list[string]",
                "created_at": "string"
              }
            ]
          },
          {
            "method": "GET",
            "path": "/cohort",
            "response_model": "CohortResult",
            "response": {
              "trait_averages": "dict[string, float]",
              "total_evaluations": "int"
            }
          }
        ]
      },
      "contextFiles": [
        "api/main.py",
        "ethos/graph/read.py",
        "ethos/graph/cohort.py",
        "ethos/graph/service.py",
        "ethos/shared/models.py",
        "ethos/__init__.py"
      ],
      "notes": "DDD layering: Create ethos/agents.py as a new domain module for agent listing, profile, history, and cohort queries. API handler calls ethos.list_agents() → agents.py calls graph/read.py:get_all_agents() → graph/read.py owns the Cypher. API NEVER touches graph directly. New Cypher in graph/read.py: MATCH (a:Agent) OPTIONAL MATCH (a)-[:EVALUATED]->(e:Evaluation) WITH a, e ORDER BY e.created_at ASC WITH a, count(e) AS evals, last(collect(e.alignment_status)) AS latest RETURN a.agent_id, evals, latest. IMPORTANT: collect() does not guarantee order in Neo4j — must ORDER BY before collect() to get the actual latest alignment_status. get_agent(agent_id) wraps existing get_agent_profile() and returns AgentProfile Pydantic model. Follow existing pattern from api/main.py where evaluate_endpoint just calls evaluate() and reflect_endpoint just calls reflect(). Also update api/main.py ReflectRequest to add text: str | None = None field (moved from TASK-007 to avoid parallel file conflict). Also add CORS middleware (FastAPI CORSMiddleware) to api/main.py allowing localhost:3000 for Academy. IMPORTANT: ethos/models.py is a re-export file that existing code imports from. When adding new models to ethos/shared/models.py, also update ethos/models.py to re-export them. Both files must expose the same set of models. BUG NOTE: existing balance.py _GET_BALANCE_VS_TRUST_QUERY references e.trust_score which is never stored on Evaluation nodes — this query returns null. If balance analysis is needed for demo, either add trust_score: Float to Evaluation node in TASK-004's write.py update (computed as (ethos+logos+pathos)/3.0), or rewrite the query to compute inline.",
      "retryCount": 0
    },
    {
      "id": "TASK-009",
      "type": "backend",
      "title": "Create insights engine — Opus temporal behavioral analysis",
      "priority": 9,
      "passes": true,
      "batch": 4,
      "dependsOn": [
        "TASK-007",
        "TASK-008"
      ],
      "techStack": {
        "backend": "Python 3.11+, Pydantic v2, anthropic SDK, neo4j sync driver",
        "testing": "pytest"
      },
      "constraints": [
        "All code is SYNC — no async/await",
        "Always use Opus (claude-opus-4-6) for insights — this is the Opus showcase",
        "Graph is optional — return InsightsResult with summary='Graph unavailable' if Neo4j is down (never crash)",
        "InsightsResult and Insight Pydantic models already exist in shared/models.py",
        "DDD: insights.py (domain) calls graph/read.py and graph/cohort.py. API calls ethos.insights(). API never touches graph directly",
        "API endpoint uses response_model=InsightsResult"
      ],
      "files": {
        "create": [
          "ethos/insights.py",
          "ethos/evaluation/insights_prompts.py"
        ],
        "modify": [
          "ethos/__init__.py",
          "api/main.py"
        ],
        "reuse": [
          "ethos/taxonomy/constitution.py",
          "ethos/evaluation/claude_client.py",
          "ethos/shared/models.py",
          "ethos/config/config.py"
        ]
      },
      "acceptanceCriteria": [
        "insights(agent_id) fetches agent history + cohort averages from graph, sends to Opus, returns InsightsResult",
        "Prompt includes agent's last 20 evaluations with timestamps, cohort averages for comparison, and sabotage pathways from constitution.py",
        "InsightsResult contains summary, list of Insight objects (trait, severity, message, evidence), and stats",
        "GET /insights/{agent_id} endpoint uses response_model=InsightsResult — fully typed Pydantic response (GET per api-specification.md — generating insights is a read operation)",
        "API handler contains ONLY: call ethos.insights(agent_id) → return result. Zero business logic in handler",
        "insights() exported from ethos/__init__.py so API imports from ethos package, not submodules"
      ],
      "errorHandling": [
        "Neo4j down → return InsightsResult with summary='Graph unavailable' and empty insights",
        "Agent not found → return InsightsResult with summary='No evaluation history found'",
        "Claude API error → return InsightsResult with summary describing the error"
      ],
      "testing": {
        "types": [
          "unit"
        ],
        "approach": "TDD",
        "runner": "pytest",
        "files": {
          "unit": [
            "tests/test_insights.py"
          ]
        }
      },
      "testSteps": [
        "uv run pytest tests/test_insights.py -v"
      ],
      "contextFiles": [
        "ethos/taxonomy/constitution.py",
        "ethos/shared/models.py",
        "ethos/graph/read.py",
        "ethos/graph/cohort.py",
        "ethos/graph/service.py",
        "api/main.py"
      ],
      "notes": "This is the Opus 4.6 showcase — worth 25% of hackathon judging. The prompt should ask Opus to: 1) Identify temporal patterns in trait scores, 2) Compare agent to cohort baseline, 3) Check for sabotage pathway matches (SP-01 through SP-08 from constitution.py), 4) Generate actionable recommendations. Each Insight should have severity: info/warning/critical. Export insights from ethos/__init__.py. IMPORTANT: Mock the Anthropic client in ALL unit tests — do NOT call the real Opus API. Opus is ~10x Sonnet cost. Use unittest.mock.patch on call_claude to return a fake JSON response. Set max_tokens to 2048 for the Opus call to prevent runaway output costs.",
      "retryCount": 0
    },
    {
      "id": "TASK-010",
      "type": "frontend",
      "title": "Initialize Academy with styleguide and layout",
      "priority": 10,
      "passes": true,
      "batch": 3,
      "dependsOn": [
        "TASK-004"
      ],
      "techStack": {
        "frontend": "Next.js 14+, TypeScript, Tailwind CSS",
        "testing": "vitest or jest"
      },
      "constraints": [
        "Use Next.js App Router",
        "Light + clean aesthetic — professional dashboard, not gaming",
        "Use Tailwind CSS for styling",
        "API base URL configurable via environment variable NEXT_PUBLIC_API_URL (default {config.urls.backend})"
      ],
      "files": {
        "create": [
          "academy/package.json",
          "academy/next.config.ts",
          "academy/tsconfig.json",
          "academy/tailwind.config.ts",
          "academy/.env.local",
          "academy/app/layout.tsx",
          "academy/app/page.tsx",
          "academy/app/globals.css",
          "academy/lib/api.ts",
          "academy/lib/types.ts"
        ],
        "modify": [],
        "reuse": [
          "ethos/shared/models.py"
        ]
      },
      "acceptanceCriteria": [
        "Next.js app initializes and renders at http://localhost:3000",
        "Layout includes header with Ethos branding and navigation placeholder",
        "API client in lib/api.ts has typed functions for evaluate(), reflect(), getAgents(), getHistory(), getCohort() with full TypeScript types matching Pydantic models",
        "Tailwind configured with Ethos design tokens (light background, professional palette)"
      ],
      "errorHandling": [
        "API unreachable → show 'API unavailable' message, not crash"
      ],
      "testing": {
        "types": [
          "unit"
        ],
        "approach": "test-after",
        "runner": "jest",
        "files": {
          "unit": []
        }
      },
      "testSteps": [
        "cd academy && npm install && npx tsc --noEmit",
        "cd academy && npm run build"
      ],
      "testUrl": "http://localhost:3000",
      "mcp": [
        "playwright",
        "devtools"
      ],
      "contextFiles": [
        "docs/evergreen-architecture/product-design.md"
      ],
      "skills": [
        {
          "name": "styleguide",
          "usage": "Generate Ethos design tokens and component styles before building UI"
        }
      ],
      "notes": "Run /styleguide first to establish design tokens. Light background (#fafafa or white), slate/gray text, teal accents for trust-positive, red/orange for trust-negative. Inter font for body. Clean, minimal — think Vercel dashboard or Linear. CORS: API needs to allow requests from localhost:3000. Transform API responses from snake_case to camelCase. Create typed interfaces with camelCase properties.",
      "retryCount": 0
    },
    {
      "id": "TASK-011",
      "type": "frontend",
      "title": "Build live evaluator panel with radar chart",
      "priority": 11,
      "passes": false,
      "batch": 4,
      "dependsOn": [
        "TASK-010"
      ],
      "techStack": {
        "frontend": "Next.js 14+, TypeScript, Tailwind CSS, Recharts",
        "testing": "jest"
      },
      "constraints": [
        "Use Recharts RadarChart for 12-trait visualization",
        "POST to /evaluate endpoint on form submit",
        "Show loading state during evaluation",
        "Color coding: green for positive traits, red for negative traits"
      ],
      "files": {
        "create": [
          "academy/components/EvaluatorPanel.tsx",
          "academy/components/RadarChart.tsx",
          "academy/components/ScoreCard.tsx"
        ],
        "modify": [
          "academy/app/page.tsx"
        ],
        "reuse": [
          "academy/lib/api.ts"
        ]
      },
      "acceptanceCriteria": [
        "Text input area with 'Evaluate' button submits text to POST /evaluate",
        "Radar chart renders 12 trait scores after evaluation completes",
        "Score cards show ethos/logos/pathos dimension scores and alignment status",
        "Trust status displayed with color coding (aligned=green, drifting=yellow, misaligned=red, violation=red)"
      ],
      "errorHandling": [
        "API timeout → show error message, keep previous result",
        "Empty text → disable submit button"
      ],
      "testing": {
        "types": [
          "e2e"
        ],
        "approach": "test-after",
        "runner": "jest",
        "files": {
          "e2e": []
        }
      },
      "testSteps": [
        "cd academy && npx tsc --noEmit",
        "cd academy && npm run build"
      ],
      "testUrl": "http://localhost:3000",
      "mcp": [
        "playwright",
        "devtools"
      ],
      "contextFiles": [
        "ethos/shared/models.py",
        "academy/lib/api.ts"
      ],
      "notes": "The radar chart is the hero visual. 12 traits arranged in a circle. Trustworthy messages produce a smooth, outward shape. Manipulative messages produce spiky, collapsed shapes on manipulation/deception axes. This should be immediately readable and visually striking. Add a few example messages as quick-fill buttons for the demo."
    },
    {
      "id": "TASK-012",
      "type": "frontend",
      "title": "Build agent timeline and cohort panels",
      "priority": 12,
      "passes": false,
      "batch": 5,
      "dependsOn": [
        "TASK-011",
        "TASK-008"
      ],
      "scale": "medium",
      "techStack": {
        "frontend": "Next.js 14+, TypeScript, Tailwind CSS, Recharts",
        "testing": "jest"
      },
      "constraints": [
        "Use Recharts LineChart for timeline",
        "Agent selector dropdown populated from GET /agents",
        "Timeline shows ethos/logos/pathos over time from GET /agent/{id}/history",
        "Cohort panel shows bar chart from GET /cohort"
      ],
      "files": {
        "create": [
          "academy/components/AgentTimeline.tsx",
          "academy/components/CohortPanel.tsx",
          "academy/components/AgentSelector.tsx"
        ],
        "modify": [
          "academy/app/page.tsx"
        ],
        "reuse": [
          "academy/lib/api.ts"
        ]
      },
      "acceptanceCriteria": [
        "Agent dropdown populated from /agents endpoint with agent IDs and evaluation counts",
        "Selecting an agent loads timeline from /agent/{id}/history showing ethos/logos/pathos line chart over time",
        "Flags appear as red dots on the timeline",
        "Cohort panel shows bar chart of 12 trait averages from /cohort endpoint"
      ],
      "errorHandling": [
        "No agents in graph → show 'No agents evaluated yet' message",
        "API error → show error state, don't crash"
      ],
      "testing": {
        "types": [
          "e2e"
        ],
        "approach": "test-after",
        "runner": "jest",
        "files": {
          "e2e": []
        }
      },
      "testSteps": [
        "cd academy && npx tsc --noEmit",
        "cd academy && npm run build"
      ],
      "testUrl": "http://localhost:3000",
      "mcp": [
        "playwright",
        "devtools"
      ],
      "contextFiles": [
        "academy/lib/api.ts",
        "ethos/shared/models.py"
      ],
      "notes": "The timeline is the Phronesis visualization — trust over time. An agent that starts trustworthy and drifts into manipulation should show a clear visual decline. Color the lines: ethos=teal, logos=blue, pathos=warm. Show trend arrow (improving/declining/stable) from reflect() data. Transform API responses from snake_case to camelCase. Create typed interfaces with camelCase properties."
    },
    {
      "id": "TASK-013",
      "type": "backend",
      "title": "Layer 3 pattern detector — deterministic graph queries for temporal patterns",
      "priority": 13,
      "passes": false,
      "batch": 5,
      "dependsOn": [
        "TASK-006",
        "TASK-009"
      ],
      "techStack": {
        "backend": "Python 3.11+, Pydantic v2, neo4j sync driver",
        "testing": "pytest"
      },
      "constraints": [
        "All code is SYNC — no async/await",
        "NO LLM calls — patterns are detected by graph queries on score sequences, not Claude",
        "Graph owns all Cypher — pattern detection queries go in ethos/graph/patterns.py",
        "Results stored as (Agent)-[:EXHIBITS_PATTERN]->(Pattern) relationships in Neo4j",
        "Graph is optional — return empty PatternResult if Neo4j is down",
        "All results returned as Pydantic models"
      ],
      "files": {
        "create": [
          "ethos/graph/patterns.py",
          "ethos/patterns.py"
        ],
        "modify": [
          "ethos/shared/models.py",
          "ethos/models.py",
          "ethos/__init__.py",
          "api/main.py"
        ],
        "reuse": [
          "ethos/taxonomy/constitution.py",
          "ethos/taxonomy/indicators.py",
          "ethos/config/config.py"
        ]
      },
      "acceptanceCriteria": [
        "New Pydantic models in shared/models.py: DetectedPattern(pattern_id, name, description, matched_indicators: list[str], confidence: float, first_seen: str, last_seen: str, occurrence_count: int, current_stage: int), PatternResult(agent_id, patterns: list[DetectedPattern], checked_at) — DetectedPattern fields match the EXHIBITS_PATTERN relationship properties in neo4j-schema.md",
        "ethos/graph/patterns.py contains Cypher queries that analyze evaluation score sequences to detect 8 sabotage pathways (SP-01 through SP-08) per framework overview",
        "detect_patterns(agent_id) in ethos/patterns.py queries graph for agent's recent evaluations, checks score sequences against pattern definitions, returns PatternResult",
        "When patterns detected, creates (Agent)-[:EXHIBITS_PATTERN {first_seen, last_seen, occurrence_count, current_stage, confidence}]->(Pattern) relationship in Neo4j per neo4j-schema.md — MERGE to update existing patterns, CREATE for new ones",
        "GET /agent/{agent_id}/patterns endpoint uses response_model=PatternResult",
        "API handler calls ethos.detect_patterns() — zero business logic in handler"
      ],
      "errorHandling": [
        "Neo4j down → return empty PatternResult",
        "Agent not found → return empty PatternResult",
        "Fewer than 5 evaluations → return PatternResult with note 'insufficient data for pattern detection'"
      ],
      "testing": {
        "types": [
          "unit",
          "integration"
        ],
        "approach": "TDD",
        "runner": "pytest",
        "files": {
          "unit": [
            "tests/test_patterns.py"
          ],
          "integration": []
        }
      },
      "testSteps": [
        "uv run pytest tests/test_patterns.py -v",
        "uv run uvicorn api.main:app --port 8917 & sleep 3 && curl -s {config.urls.backend}/agent/test-agent/patterns | python3 -c \"import sys, json; r=json.load(sys.stdin); assert 'patterns' in r; print('PASS: /patterns returns PatternResult')\" ; kill %1 2>/dev/null"
      ],
      "apiContract": {
        "endpoint": "GET /agent/{agent_id}/patterns",
        "response_model": "PatternResult",
        "response": {
          "agent_id": "string",
          "patterns": [
            {
              "pattern_id": "string",
              "name": "string",
              "confidence": "float",
              "matched_indicators": "list[string]",
              "first_seen": "string",
              "last_seen": "string",
              "occurrence_count": "int",
              "current_stage": "int"
            }
          ],
          "checked_at": "string"
        }
      },
      "contextFiles": [
        "docs/evergreen-architecture/ethos-framework-overview.md",
        "ethos/taxonomy/constitution.py",
        "ethos/taxonomy/indicators.py",
        "ethos/graph/read.py"
      ],
      "notes": "This is Layer 3 from the framework overview. NO LLM — pure graph queries. The detection logic: 1) Fetch agent's last N evaluations ordered by time, 2) For each sabotage pathway, traverse (:Pattern)-[:COMPOSED_OF]->(:Indicator) to get the mapped indicators (seeded by TASK-006), then check if those indicators appear via (:Evaluation)-[:DETECTED]->(:Indicator) in recent evaluations, 3) For temporal patterns (love bombing = high manipulation followed by high compassion), check score sequences over sliding windows. The (:Pattern)-[:COMPOSED_OF]->(:Indicator) relationships from TASK-006 make this fully graph-native — no need to import Python data structures at query time. EXHIBITS_PATTERN uses MERGE so repeated detection updates occurrence_count and last_seen rather than creating duplicates. Start with the 8 sabotage pathways (SP-01 through SP-08). This complements TASK-009 (Opus insights) — pattern detector finds the patterns deterministically, insights engine explains what they mean."
    }
  ]
}
