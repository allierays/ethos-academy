# AI Regulatory Landscape: The Case for Ethos

> Regulations are arriving. Tools to comply with them are not. Ethos fills the gap.

**Last updated:** February 2026

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [The EU AI Act](#the-eu-ai-act)
3. [NIST AI Risk Management Framework (AI RMF 1.0)](#nist-ai-risk-management-framework)
4. [NIST AI 600-1: Generative AI Risk Profile](#nist-ai-600-1-generative-ai-risk-profile)
5. [US Executive Order on AI Safety (EO 14110)](#us-executive-order-on-ai-safety)
6. [AI Agent-Specific Regulation](#ai-agent-specific-regulation)
7. [ISO/IEC 42001: AI Management Systems](#isoiec-42001-ai-management-systems)
8. [US State-Level AI Laws](#us-state-level-ai-laws)
9. [The Compliance Gap](#the-compliance-gap)
10. [How Ethos Maps to Compliance](#how-ethos-maps-to-compliance)
11. [Regulation-to-Ethos Mapping Table](#regulation-to-ethos-mapping-table)
12. [Sources](#sources)

---

## Executive Summary

2026 is the year that AI regulation stops being theoretical and starts being enforceable. The EU AI Act's high-risk system requirements take effect in August 2026. Colorado's AI Act begins enforcement in June 2026. Singapore launched the world's first governance framework for agentic AI in January 2026. The World Economic Forum reports that 82% of executives plan to deploy AI agents within one to three years, but the gap between adoption and oversight tools is widening.

The regulatory consensus is clear: AI systems need continuous monitoring, transparent audit trails, risk scoring, and human oversight. But the tooling to deliver these requirements -- particularly for AI agents operating autonomously and communicating with other agents -- barely exists.

Ethos is built for exactly this moment. It provides continuous evaluation of AI agent behavior across trust, accuracy, and emotional manipulation dimensions. It stores every evaluation in a Neo4j knowledge graph, creating the audit trail regulators will demand. It detects patterns across agent networks over time. And it is open source, providing the transparency that every major regulatory framework treats as a baseline requirement.

This document maps each major regulation to the specific capabilities Ethos provides, demonstrating that Ethos is not just a hackathon project -- it is infrastructure for the regulatory reality that is already here.

---

## The EU AI Act

The [EU Artificial Intelligence Act](https://artificialintelligenceact.eu/high-level-summary/) is the world's first comprehensive AI regulation. It entered into force on August 1, 2024, with provisions phasing in over a three-year timeline.

### Risk Classification Tiers

The Act classifies AI systems into four risk categories:

| Tier | Description | Regulatory Treatment |
|------|-------------|---------------------|
| **Unacceptable Risk** | Social scoring, real-time biometric surveillance, manipulation of vulnerable groups | Banned outright |
| **High Risk** | AI in hiring, credit scoring, education, law enforcement, critical infrastructure | Subject to extensive requirements (Articles 9-15) |
| **Limited Risk** | Chatbots, AI-generated content, emotion recognition | Transparency obligations (Article 50) |
| **Minimal Risk** | Spam filters, AI in video games | No specific obligations |

### Implementation Timeline

| Date | What Takes Effect |
|------|-------------------|
| **February 2, 2025** | Prohibitions on unacceptable-risk AI practices |
| **August 2, 2025** | Obligations for providers of general-purpose AI (GPAI) models; GPAI Code of Practice |
| **February 2, 2026** | Commission guidelines on high-risk classification; practical examples published |
| **August 2, 2026** | Full high-risk AI system requirements (Articles 9-15); transparency obligations (Article 50); post-market monitoring; conformity assessments; market surveillance; Commission enforcement powers with fines |
| **August 2, 2027** | Compliance deadline for GPAI models placed on market before August 2025 |

Note: The Commission proposed a Digital Omnibus package in November 2025 that could delay some high-risk obligations by 16 months (to December 2027), but this is not yet enacted.

### High-Risk System Requirements (Articles 9-15)

These are the obligations that take effect August 2, 2026:

- **Risk Management System (Article 9):** A continuous, iterative process throughout the AI system's entire lifecycle. Must identify and evaluate risks during intended use and foreseeable misuse. Must integrate post-market monitoring data. Must adopt targeted risk mitigation measures.

- **Data Governance (Article 10):** Training, validation, and testing data must meet quality criteria. Data sets must be relevant, representative, and free of errors.

- **Technical Documentation (Article 11):** Detailed documentation of the system's design, development, and capabilities, maintained before market placement and kept up to date.

- **Record-Keeping (Article 12):** Automatic logging of system operations. Logs must enable traceability of the system's operation throughout its lifecycle.

- **Transparency (Article 13):** Systems must be designed to enable deployers to interpret outputs and use them appropriately.

- **Human Oversight (Article 14):** Systems must be designed to allow effective human oversight, including the ability to understand the system's capabilities and limitations, properly monitor operation, and intervene or interrupt.

- **Accuracy, Robustness, and Cybersecurity (Article 15):** Systems must achieve appropriate levels of accuracy, be resilient to errors and inconsistencies, and be protected against unauthorized access.

### Transparency Obligations (Article 50)

Applicable to all AI systems that interact directly with humans:

- People must be informed they are interacting with an AI system (unless it is obvious from context).
- AI-generated synthetic content (audio, image, video, text) must be marked in machine-readable format and detectable as artificially generated.
- Information must be provided clearly and distinguishably at the time of first interaction.

### GPAI Model Obligations

Providers of general-purpose AI models (the foundation models that power agent systems) must:

- Maintain detailed technical documentation
- Publish summaries of training data
- Comply with EU copyright law
- Share information with regulators and downstream users
- Non-compliance fines: up to 15 million EUR or 3% of global revenue

### What This Means for AI Agents

The EU AI Act does not have a separate "AI agent" category, but agents are captured through multiple provisions:

- Agents that make or influence **consequential decisions** about people (hiring, credit, education) qualify as high-risk systems under Annex III.
- Agents that interact with people are subject to **Article 50 transparency** requirements.
- The underlying models powering agents are subject to **GPAI obligations**.
- The **post-market monitoring** requirement (Article 72) demands ongoing surveillance of deployed systems -- precisely the kind of continuous evaluation Ethos provides.

### Ethos Relevance

Ethos directly addresses: risk management (continuous scoring), record-keeping (Neo4j graph logging), transparency (open-source evaluation logic), human oversight (trust scores that inform human decisions), and post-market monitoring (longitudinal agent behavior tracking).

---

## NIST AI Risk Management Framework

The [NIST AI Risk Management Framework (AI RMF 1.0)](https://www.nist.gov/itl/ai-risk-management-framework), published as NIST AI 100-1, is a voluntary framework for managing risks throughout the AI system lifecycle. While not legally binding, it has become the de facto standard in the US and is referenced by multiple state laws and federal procurement requirements.

### Four Core Functions

The framework is built on four interconnected, iterative functions:

**1. GOVERN**
Cultivate a risk-aware organizational culture. Establish governance structures, policies, and accountability mechanisms. Define roles and responsibilities for AI risk management. Leadership commitment is the foundation.

**2. MAP**
Contextualize AI systems within their broader operational environment. Identify potential impacts across technical, social, and ethical dimensions. Understand the intended and unintended consequences of AI deployment. Map stakeholders and affected communities.

**3. MEASURE**
Assess risks through both quantitative and qualitative approaches. Evaluate the likelihood and potential consequences of AI-related risks. Track metrics over time. Use appropriate tools and methodologies for the context.

**4. MANAGE**
Prioritize and address identified risks. Implement response strategies. Allocate resources based on risk severity. Monitor effectiveness of risk treatments. Communicate risk information to stakeholders.

These functions are not sequential steps -- they form a continuous cycle throughout an AI system's lifecycle.

### Seven Characteristics of Trustworthy AI

The AI RMF identifies seven characteristics that trustworthy AI systems must exhibit:

| Characteristic | Description |
|---------------|-------------|
| **Valid and Reliable** | Consistently performs as intended under expected and unexpected conditions |
| **Safe** | Does not cause harm to people, property, or the environment |
| **Secure and Resilient** | Resistant to attacks and able to recover from disruptions |
| **Accountable and Transparent** | Clear responsibility chains; operations visible to stakeholders |
| **Explainable and Interpretable** | Outputs can be understood and reasoned about by humans |
| **Privacy-Enhanced** | Protects personal data and individual autonomy |
| **Fair â€” with Harmful Bias Managed** | Does not discriminate; bias is identified and mitigated |

Creating trustworthy AI requires balancing these characteristics based on the system's context of use. No single characteristic takes absolute priority.

### Ethos Relevance

Ethos maps directly to the NIST framework:

- **GOVERN:** Ethos provides the scoring infrastructure that governance policies can reference. Organizations can define trust thresholds and alert on violations.
- **MAP:** The Neo4j knowledge graph maps agent relationships, contexts, and potential impact zones across an agent network.
- **MEASURE:** Ethos scores (ethos, logos, pathos, trust, compassion, honesty, accuracy) are quantitative risk measures computed continuously.
- **MANAGE:** The reflection system detects trends (improving, declining, stable) enabling proactive risk management before harm occurs.

For the seven characteristics: Ethos evaluates for reliability (consistency scoring), safety (manipulation detection), transparency (open-source logic), explainability (structured evaluation outputs), and fairness (bias flags in evaluation).

---

## NIST AI 600-1: Generative AI Risk Profile

[NIST AI 600-1](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf), released July 26, 2024, is a companion document to the AI RMF specifically addressing risks unique to or exacerbated by generative AI. It was developed pursuant to Executive Order 14110 and informed by the Generative AI Public Working Group (GAI PWG).

### Twelve GAI Risk Categories

The profile identifies twelve risk categories with over 200 suggested mitigation actions:

| # | Risk Category | Description |
|---|--------------|-------------|
| 1 | **CBRN Information** | Access to weapons of mass destruction design or synthesis information |
| 2 | **Confabulation** | Generation of false, fabricated, or hallucinated content presented as fact |
| 3 | **Data Privacy** | Leakage of personal or sensitive data through AI inputs or outputs |
| 4 | **Environmental Impact** | High compute demands causing ecological harm |
| 5 | **Dangerous Content** | Generation of content that incites violence, self-harm, or criminal activity |
| 6 | **Hate and Bias** | Production of hateful, disparaging, or stereotyping content |
| 7 | **Obscene/Abusive Content** | Generation of offensive, degrading, or non-consensual content |
| 8 | **Information Integrity** | Undermining trust in information ecosystems (misinformation, deepfakes) |
| 9 | **Information Security** | Vulnerabilities to adversarial attacks, prompt injection, data poisoning |
| 10 | **Intellectual Property** | Copyright infringement, unauthorized use of protected material |
| 11 | **Value Chain Integration** | Third-party component risks obscured through complex supply chains |
| 12 | **Homogenization** | Reduced diversity of outputs, monoculture risks across AI ecosystem |

### Four Focus Areas

The profile emphasizes four cross-cutting concerns:

1. **Governance** -- Organizational structures for GAI risk oversight
2. **Content Provenance** -- Tracking the origin and authenticity of AI-generated content
3. **Pre-deployment Testing** -- Evaluation before release
4. **Incident Disclosure** -- Reporting mechanisms when things go wrong

### Ethos Relevance

Ethos directly addresses several of these risk categories:

- **Confabulation (Risk 2):** The logos score evaluates logical coherence, evidence quality, and reasoning -- directly detecting hallucinated or unsupported claims.
- **Information Integrity (Risk 8):** The ethos score measures credibility and authority; the pathos score flags emotional manipulation. Together they detect when an agent is producing misleading content.
- **Information Security (Risk 9):** By evaluating agent outputs continuously, Ethos can detect behavioral shifts that indicate prompt injection or adversarial compromise.
- **Value Chain Integration (Risk 11):** The Neo4j graph tracks agent interactions across a network, making third-party agent behavior visible and auditable.
- **Pre-deployment Testing:** Ethos evaluation can be integrated into CI/CD pipelines as a pre-deployment gate.
- **Incident Disclosure:** The graph audit trail provides the evidence needed for incident reports.

---

## US Executive Order on AI Safety

### Executive Order 14110 (October 30, 2023)

The Biden administration's [Executive Order on Safe, Secure, and Trustworthy AI](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/) was the most comprehensive US federal action on AI governance. Key provisions included:

- Required developers of powerful AI models to share safety test results with the US government
- Directed NIST to develop standards for AI red-teaming and evaluation
- Required watermarking of AI-generated content
- Established new standards for biological synthesis screening
- Directed agencies to address AI risks in critical infrastructure
- Required assessment of AI's impact on labor markets
- Promoted responsible AI innovation while managing risks

### Current Status (2025-2026)

**EO 14110 was rescinded on January 20, 2025**, within hours of President Trump taking office. The replacement approach emphasizes:

- **Innovation over regulation:** The new executive order, "Removing Barriers to American Leadership in Artificial Intelligence," focuses on reducing regulatory burden on AI development.
- **Infrastructure investment:** A $500 billion private-sector AI infrastructure initiative (Stargate) was announced.
- **State-level preemption:** A December 2025 executive order placed existing state-level AI laws (California, Colorado, Texas, Utah) under scrutiny, potentially limiting state regulatory authority.

### Why This Matters for Ethos

The rescission of EO 14110 does **not** reduce the relevance of AI safety tooling. It increases it:

1. **Federal regulatory vacuum:** With federal oversight pulled back, the burden falls on organizations to self-govern. Ethos provides the infrastructure for voluntary but rigorous self-governance.
2. **State laws remain:** Colorado's AI Act, California's various AI bills, and similar state legislation still require compliance tooling.
3. **EU extraterritorial reach:** Any organization serving EU customers must comply with the EU AI Act regardless of US federal policy.
4. **NIST frameworks survive:** The AI RMF and AI 600-1 were developed by NIST as voluntary standards. They were not rescinded and continue to serve as the primary US technical guidance.
5. **Market demand:** Enterprises, investors, and customers increasingly require AI governance regardless of government mandates. Trust is a competitive differentiator.

---

## AI Agent-Specific Regulation

AI agents -- systems that autonomously plan, reason, and execute actions -- represent a regulatory frontier. As of early 2026, we are seeing the first dedicated governance frameworks emerge.

### Singapore: Model AI Governance Framework for Agentic AI (January 2026)

[Singapore's IMDA launched the world's first governance framework for agentic AI](https://www.imda.gov.sg/resources/press-releases-factsheets-and-speeches/press-releases/2026/new-model-ai-governance-framework-for-agentic-ai) on January 22, 2026, at the World Economic Forum in Davos. The framework addresses four core dimensions:

**1. Risk Assessment and Bounding**
- Conduct use-case-specific assessments considering autonomy level, data sensitivity, and action scope
- Bound risks by design: limit tool access, permissions, operational environments, and the scope of actions agents may take

**2. Human Accountability**
- Allocate clear responsibilities across the AI lifecycle: developers, deployers, operators, end users
- Implement human oversight mechanisms that can override, intercept, or review agent actions

**3. Technical Controls**
- Design phase: tool guardrails, plan reflections, least-privilege access
- Pre-deployment: test task execution, policy compliance, tool-use accuracy across varied data sets
- Post-deployment: staged rollouts, real-time monitoring

**4. End-User Responsibility**
- Inform users of agent capabilities and limitations
- Provide escalation paths for malfunction
- Maintain essential human skills through training

Compliance is voluntary, but organizations remain legally accountable for their agents' behaviors.

### World Economic Forum: AI Agents in Action (November 2025)

The [WEF framework](https://www.weforum.org/publications/ai-agents-in-action-foundations-for-evaluation-and-governance/) provides three pillars for agent governance:

**1. Classification:** Create an "agent card" or "resume" for each AI agent documenting function, autonomy level, authority scope, use case, and operational complexity.

**2. Evaluation:** Assess task success rates, tool-use reliability, performance over time, user trust, and operational robustness in real-world settings.

**3. Risk Assessment:** Translate evaluation evidence into safety and suitability insights.

The WEF identifies key multi-agent system challenges:
- **Semantic misalignment:** Agents interpret instructions differently
- **Security and trust gaps:** Between agents in a network
- **Cascading effects:** Failures propagating across interconnected agents
- **Systemic complexity:** Growing unpredictably as agent networks scale

The baseline governance recommendation for all agents includes: **logging and traceability, clear identity tagging for every agent action, and real-time monitoring.**

### The Accountability Gap

A central problem in AI agent regulation is what scholars call the "accountability gap":

- Agents operate with autonomy that distances the original human instruction from the final output
- When harm occurs, developers, deployers, data curators, and executives each disclaim responsibility
- Courts have not yet issued definitive rulings on fully autonomous agent liability
- The EU's Product Liability Directive (to be implemented by December 2026) explicitly includes software and AI as "products," enabling strict liability if an AI system is found "defective"
- California's AB 316 (effective January 1, 2026) precludes defendants from using an AI system's autonomous operation as a defense to liability claims

### Ethos Relevance

Ethos is purpose-built for the agent governance requirements emerging across all these frameworks:

- **Agent identity and tracking:** Every evaluation in Ethos is linked to a specific agent ID, stored in the graph with full provenance
- **Real-time monitoring:** Continuous evaluation of agent outputs against trust, accuracy, and manipulation metrics
- **Audit trail:** The Neo4j graph provides exactly the logging and traceability that every framework demands as a baseline
- **Multi-agent visibility:** The graph structure naturally models agent-to-agent relationships, enabling detection of cascading failures and semantic drift across networks
- **Trend detection:** The reflection system identifies behavioral changes over time, supporting the staged rollout and progressive oversight models recommended by Singapore and WEF

---

## ISO/IEC 42001: AI Management Systems

[ISO/IEC 42001:2023](https://www.iso.org/standard/42001) is the world's first international standard for AI management systems (AIMS). Published in December 2023, it provides a certifiable framework for organizations to manage AI responsibly.

### Purpose

The standard specifies requirements for establishing, implementing, maintaining, and continually improving an AI Management System. It uses the Plan-Do-Check-Act (PDCA) methodology familiar from other ISO management system standards (like ISO 27001 for information security).

### Key Requirements

- **Governance:** Structured framework for governing AI projects, models, and data
- **Risk Management:** Systematic identification and treatment of AI-related risks and opportunities
- **Responsible AI:** Involvement of compliance teams, AI developers, and risk professionals in decision-making
- **Continuous Improvement:** Ongoing assessment and enhancement of AI governance practices
- **Ethical Principles:** Fairness, non-discrimination, respect for privacy, transparency, and accountability embedded in processes

### Certification

- Certification is granted by independent third-party auditors
- Valid for three years with annual surveillance audits
- Re-certification required in year three
- Major organizations (Microsoft, AWS) have achieved certification

### Ethos Relevance

ISO 42001 requires organizations to demonstrate continuous monitoring and improvement of AI systems. Ethos provides:

- **Monitoring evidence:** Continuous evaluation scores provide quantitative evidence for audit
- **Risk metrics:** Trust, accuracy, and manipulation scores map directly to risk indicators
- **Improvement tracking:** The reflection system shows trends over time, demonstrating the "Check" and "Act" phases of PDCA
- **Documentation:** Graph data exports serve as technical documentation of system behavior
- **Transparency:** Open-source codebase enables auditors to inspect evaluation logic directly

---

## US State-Level AI Laws

With federal regulation pulled back, US states are leading AI governance. Two laws are particularly relevant:

### Colorado AI Act (SB 24-205)

**Effective:** June 30, 2026

The [Colorado AI Act](https://leg.colorado.gov/bills/sb24-205) is the first comprehensive US state law regulating high-risk AI systems.

**Key Requirements:**
- **Duty of care:** Developers and deployers must exercise reasonable care to protect consumers from algorithmic discrimination
- **Impact assessments:** Required within 90 days of effective date, then annually, and within 90 days of any substantial modification
- **Risk management program:** Must implement policies and programs governing high-risk AI deployment
- **Transparency:** Consumers must be notified when interacting with AI and when AI makes decisions adverse to their interests
- **Correction rights:** Consumers can correct incorrect personal data processed by high-risk AI
- **Record retention:** Four years

**Ethos relevance:** Ethos evaluation data provides the continuous risk assessment and audit documentation Colorado requires. The graph-based history supports annual impact assessments and demonstrates ongoing monitoring.

### California AB 316 (Effective January 1, 2026)

Precludes defendants from using an AI system's autonomous operation as a defense to liability. This means organizations deploying AI agents in California are fully liable for agent actions -- making monitoring and audit trails essential.

### OMB M-26-04 (December 2025)

Requires federal agencies purchasing large language models to request model cards, evaluation artifacts, and acceptable use policies by March 2026. This creates procurement-driven demand for evaluation infrastructure.

---

## The Compliance Gap

### The Problem

Regulations are arriving on a defined timeline. The tooling to comply with them is not keeping pace.

**What regulators require:**

- Continuous monitoring of AI system behavior in production
- Risk scoring and classification
- Audit trails with full traceability
- Pattern detection across system networks
- Human-readable explanations of AI decisions
- Post-market surveillance and incident reporting
- Transparency into evaluation methodology

**What organizations currently have:**

- Point-in-time evaluations (pre-deployment benchmarks that do not monitor production behavior)
- Manual review processes that do not scale
- Logging systems designed for traditional software, not AI behavioral patterns
- No standardized trust metrics across agent systems
- No graph-based relationship tracking between agents
- Closed-source evaluation tools that cannot satisfy transparency requirements

### The Gap by Regulation

| Regulation | Requires | Current State of Tooling |
|-----------|----------|--------------------------|
| EU AI Act (Aug 2026) | Continuous risk management, post-market monitoring, audit logging | Most tools offer pre-deployment benchmarks only |
| NIST AI RMF | Measure function: quantitative + qualitative risk metrics over time | No standard scoring system for agent trustworthiness |
| NIST AI 600-1 | Confabulation detection, content provenance, incident disclosure | Hallucination detection is fragmented and model-specific |
| Singapore Agentic AI Framework | Real-time monitoring, agent identity tracking, least-privilege controls | No open-source agent monitoring infrastructure |
| ISO 42001 | Continuous improvement evidence, risk metrics, audit documentation | Audit documentation is manual and retroactive |
| Colorado AI Act | Annual impact assessments, risk management programs, transparency | Assessment tooling does not exist for agent-specific risks |
| WEF Agent Governance | Agent cards, evaluation across dimensions, trend tracking | No standardized agent evaluation framework |

### Why Open Source Matters

Every major regulatory framework -- the EU AI Act, NIST AI RMF, ISO 42001, Singapore's Agentic AI Framework -- emphasizes **transparency** as a foundational requirement. Closed-source evaluation tools create a paradox: you cannot demonstrate transparent AI governance using opaque governance tooling.

Ethos is open source by design. The evaluation logic, scoring methodology, and data models are all inspectable. Auditors, regulators, and the public can verify exactly how trustworthiness is assessed. This is not a philosophical choice -- it is a compliance requirement.

---

## How Ethos Maps to Compliance

Ethos provides five core capabilities that map directly to regulatory requirements:

### 1. Continuous Monitoring of AI Agent Behavior

Every message from an AI agent can be evaluated in real time across three dimensions:
- **Ethos (credibility):** Is the agent presenting itself with appropriate authority? Is it citing sources? Is it transparent about limitations?
- **Logos (reasoning):** Is the argument logically coherent? Is evidence presented accurately? Are claims supported?
- **Pathos (emotional appeal):** Is the agent using emotional manipulation? Is sentiment balanced? Are vulnerability triggers being exploited?

**Regulatory mapping:** EU AI Act Article 9 (risk management system), Article 72 (post-market monitoring), NIST AI RMF Measure function, Singapore Framework real-time monitoring, Colorado AI Act risk management program, ISO 42001 continuous monitoring.

### 2. Trust Scoring Across Multiple Dimensions

Ethos produces structured scores:
- `ethos`, `logos`, `pathos` -- float scores from 0.0 to 1.0
- `trust` -- aggregated classification: high, medium, low, or unknown
- `compassion`, `honesty`, `accuracy` -- behavioral dimensions tracked over time via reflection
- `trend` -- improving, declining, or stable

**Regulatory mapping:** NIST AI RMF seven characteristics of trustworthy AI, NIST AI 600-1 confabulation detection, WEF evaluation dimensions, Colorado AI Act impact assessment metrics.

### 3. Audit Trail via Neo4j Knowledge Graph

Every evaluation is stored as a node in a Neo4j graph database, connected to:
- The agent that produced the message
- The context in which it was produced
- Previous evaluations of the same agent
- Other agents in the same network

This creates a complete, queryable, time-series audit trail of agent behavior.

**Regulatory mapping:** EU AI Act Article 12 (record-keeping), Article 72 (post-market monitoring), ISO 42001 audit documentation, Colorado AI Act record retention (4 years), Singapore Framework logging and traceability, WEF clear identity tagging.

### 4. Pattern Detection Across Agent Networks

The graph structure enables queries that no flat logging system can support:
- Which agents show declining trust over time?
- Are there clusters of agents exhibiting similar behavioral shifts?
- When one agent's behavior degrades, do connected agents follow?
- What is the trust profile of an entire agent network?

**Regulatory mapping:** WEF multi-agent system safety (cascading effects, systemic complexity), Singapore Framework risk assessment, NIST AI RMF Map function (contextualizing systems in broader environment), EU AI Act post-market monitoring.

### 5. Open-Source Transparency

The entire evaluation pipeline is open source:
- Scoring logic is inspectable
- Prompt templates are readable
- Data models are documented (Pydantic schemas)
- Graph structure is queryable
- No black-box components

**Regulatory mapping:** EU AI Act Article 13 (transparency), Article 50 (transparency obligations), NIST AI RMF accountable and transparent characteristic, ISO 42001 responsible AI principles, Singapore Framework end-user responsibility (informing users of capabilities).

---

## Regulation-to-Ethos Mapping Table

| Regulation | Requirement | Ethos Capability | How |
|-----------|-------------|------------------|-----|
| **EU AI Act Art. 9** | Continuous risk management system | Continuous evaluation | Every agent message scored in real time across ethos/logos/pathos |
| **EU AI Act Art. 12** | Automatic logging and record-keeping | Neo4j audit graph | Every evaluation stored as graph node with full provenance |
| **EU AI Act Art. 13** | Transparency and interpretability | Open-source scoring | Evaluation logic, prompts, and models are fully inspectable |
| **EU AI Act Art. 14** | Human oversight capability | Trust scores for humans | Scores inform human decisions; humans retain final authority |
| **EU AI Act Art. 15** | Accuracy and robustness | Accuracy and reliability scoring | Logos dimension evaluates evidence quality and reasoning soundness |
| **EU AI Act Art. 50** | Disclosure of AI interaction | Agent identity in graph | Every evaluation links to agent ID; interaction context preserved |
| **EU AI Act Art. 72** | Post-market monitoring | Longitudinal tracking | Reflection system tracks agent behavior trends over deployment lifetime |
| **NIST AI RMF -- Govern** | Risk-aware culture and governance | Policy-referenceable scores | Organizations define trust thresholds; Ethos enforces them |
| **NIST AI RMF -- Map** | Contextual risk identification | Knowledge graph topology | Graph maps agent relationships, contexts, and impact zones |
| **NIST AI RMF -- Measure** | Quantitative risk assessment | Structured scoring (0.0-1.0) | ethos, logos, pathos, trust, compassion, honesty, accuracy metrics |
| **NIST AI RMF -- Manage** | Risk response and prioritization | Trend detection and alerting | Reflection identifies declining agents; enables proactive response |
| **NIST Trustworthy AI: Valid/Reliable** | Consistent intended performance | Consistency scoring over time | Reflection tracks behavioral stability across evaluations |
| **NIST Trustworthy AI: Safe** | No harm to people | Manipulation detection | Pathos score flags emotional exploitation and vulnerability targeting |
| **NIST Trustworthy AI: Accountable/Transparent** | Clear responsibility, visible operations | Open source + audit graph | Full traceability from agent to evaluation to score |
| **NIST Trustworthy AI: Explainable** | Outputs understandable by humans | Structured evaluation results | EvaluationResult includes dimension scores, flags, and trust level |
| **NIST Trustworthy AI: Fair** | Bias managed | Bias flag detection | Evaluation flags discriminatory or stereotyping content |
| **NIST AI 600-1: Confabulation** | Detect hallucinated content | Logos score | Evaluates logical coherence, evidence quality, and factual grounding |
| **NIST AI 600-1: Information Integrity** | Protect information ecosystem | Ethos + Pathos scores | Detects misinformation (credibility) and manipulation (emotional) |
| **NIST AI 600-1: Content Provenance** | Track content origin | Graph provenance chain | Every evaluation traces to source agent, timestamp, and context |
| **NIST AI 600-1: Incident Disclosure** | Report failures and harms | Graph-based evidence | Audit trail provides evidence package for incident reports |
| **NIST AI 600-1: Value Chain** | Third-party component visibility | Multi-agent graph | Tracks behavior across agent networks, including third-party agents |
| **Singapore Agentic AI: Risk Assessment** | Use-case-specific risk evaluation | Configurable evaluation | Scoring dimensions adapt to agent context and use case |
| **Singapore Agentic AI: Human Accountability** | Override and review mechanisms | Human-in-the-loop scoring | Trust scores inform humans; reflection provides decision support |
| **Singapore Agentic AI: Technical Controls** | Real-time monitoring, least privilege | Continuous evaluation pipeline | Evaluates every agent output; flags violations in real time |
| **Singapore Agentic AI: End-User Responsibility** | Transparency about agent capabilities | Open evaluation results | Users can inspect how trustworthiness was assessed |
| **WEF: Agent Classification** | Agent card / resume | Agent nodes in graph | Each agent has a graph identity with accumulated evaluation history |
| **WEF: Evaluation** | Task success, trust, robustness | Multi-dimensional scoring | ethos/logos/pathos + reflection dimensions + trend tracking |
| **WEF: Multi-Agent Safety** | Cascading failure detection | Network graph analysis | Graph queries detect behavioral contagion across agent networks |
| **ISO 42001: Continuous Improvement** | PDCA evidence | Trend tracking over time | Reflection shows improvement/decline; supports Check and Act phases |
| **ISO 42001: Risk Management** | Systematic risk identification | Structured risk scores | Evaluation dimensions map to risk categories |
| **ISO 42001: Audit Evidence** | Third-party auditable documentation | Exportable graph data | Neo4j data exports provide complete audit documentation |
| **Colorado AI Act: Impact Assessment** | Annual + modification-triggered assessments | Historical evaluation data | Graph data supports comprehensive impact assessments at any point |
| **Colorado AI Act: Risk Management Program** | Documented risk policies and programs | Evaluation-as-infrastructure | Ethos integrates into organizational risk management workflows |
| **Colorado AI Act: Transparency** | Consumer notification of AI interaction | Agent identity tracking | Every interaction logged with agent identity and evaluation results |
| **Colorado AI Act: Record Retention** | Four-year retention | Graph database persistence | Neo4j provides durable, queryable storage well beyond four years |

---

## Sources

### EU AI Act
- [EU AI Act High-Level Summary](https://artificialintelligenceact.eu/high-level-summary/)
- [EU AI Act Implementation Timeline](https://artificialintelligenceact.eu/implementation-timeline/)
- [Article 9: Risk Management System](https://artificialintelligenceact.eu/article/9/)
- [Article 50: Transparency Obligations](https://artificialintelligenceact.eu/article/50/)
- [Article 72: Post-Market Monitoring](https://artificialintelligenceact.eu/article/72/)
- [EU AI Act GPAI Guidelines Overview](https://artificialintelligenceact.eu/gpai-guidelines-overview/)
- [EU AI Act 2026 Compliance Guide](https://www.digitalapplied.com/blog/eu-ai-act-2026-compliance-european-business-guide)
- [Comprehensive EU AI Act Summary - January 2026 Update](https://www.softwareimprovementgroup.com/blog/eu-ai-act-summary/)
- [EU AI Act Risk Classifications](https://www.trail-ml.com/blog/eu-ai-act-how-risk-is-classified)

### NIST AI Risk Management Framework
- [NIST AI RMF Overview](https://www.nist.gov/itl/ai-risk-management-framework)
- [NIST AI 100-1 (PDF)](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
- [AI RMF Core Functions](https://airc.nist.gov/airmf-resources/airmf/5-sec-core/)
- [AI Risks and Trustworthiness Characteristics](https://airc.nist.gov/airmf-resources/airmf/3-sec-characteristics/)
- [NIST AI RMF Guide - RSI Security](https://blog.rsisecurity.com/nist-ai-risk-management-framework-guide/)

### NIST AI 600-1
- [NIST AI 600-1 (PDF)](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf)
- [NIST AI 600-1 Publication Page](https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-generative-artificial-intelligence)
- [NIST AI 600-1 and AI RMF - CipherNorth](https://www.ciphernorth.com/blog/nist-ai-risk-management-framework-rmf)

### US Executive Order and Federal Policy
- [Executive Order 14110 (Original)](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/)
- [EO 14110 - Wikipedia](https://en.wikipedia.org/wiki/Executive_Order_14110)
- [Removing Barriers to American Leadership in AI](https://www.whitehouse.gov/presidential-actions/2025/01/removing-barriers-to-american-leadership-in-artificial-intelligence/)
- [December 2025 Executive Order Analysis - Sidley](https://datamatters.sidley.com/2025/12/23/unpacking-the-december-11-2025-executive-order-ensuring-a-national-policy-framework-for-artificial-intelligence/)
- [2023 AI Safety EO Revoked - Quarles](https://www.quarles.com/newsroom/publications/2023-ai-safety-executive-order-revoked-and-what-lies-ahead-for-ai)

### AI Agent Governance
- [Singapore Agentic AI Framework - IMDA](https://www.imda.gov.sg/resources/press-releases-factsheets-and-speeches/press-releases/2026/new-model-ai-governance-framework-for-agentic-ai)
- [Singapore Framework - Baker McKenzie](https://www.bakermckenzie.com/en/insight/publications/2026/01/singapore-governance-framework-for-agentic-ai-launched)
- [Singapore Framework PDF](https://www.imda.gov.sg/-/media/imda/files/about/emerging-tech-and-research/artificial-intelligence/mgf-for-agentic-ai.pdf)
- [WEF: AI Agents in Action](https://www.weforum.org/publications/ai-agents-in-action-foundations-for-evaluation-and-governance/)
- [WEF: Multi-Agent Systems Safety](https://www.weforum.org/stories/2025/01/ai-agents-multi-agent-systems-safety/)
- [AI Agent Accountability Crisis - Medium](https://tahir-yamin.medium.com/the-ai-agent-accountability-crisis-3917e5b3be85)
- [Agentic AI Revolution: Managing Legal Risks - Squire Patton Boggs](https://www.squirepattonboggs.com/insights/publications/the-agentic-ai-revolution-managing-legal-risks/)
- [Autonomous AI Responsibility - Global Legal Insights](https://www.globallegalinsights.com/practice-areas/ai-machine-learning-and-big-data-laws-and-regulations/autonomous-ai-who-is-responsible-when-ai-acts-autonomously-and-things-go-wrong/)

### ISO/IEC 42001
- [ISO/IEC 42001:2023 - ISO Official](https://www.iso.org/standard/42001)
- [ISO 42001 - KPMG](https://kpmg.com/ch/en/insights/artificial-intelligence/iso-iec-42001.html)
- [ISO 42001 - A-LIGN](https://www.a-lign.com/articles/understanding-iso-42001)
- [ISO 42001 - Microsoft Compliance](https://learn.microsoft.com/en-us/compliance/regulatory/offering-iso-42001)

### US State AI Laws
- [Colorado AI Act (SB 24-205)](https://leg.colorado.gov/bills/sb24-205)
- [Colorado AI Act Compliance Guide](https://almcorp.com/blog/colorado-ai-act-sb-205-compliance-guide/)
- [Colorado AI Act Deep Dive - NAAG](https://www.naag.org/attorney-general-journal/a-deep-dive-into-colorados-artificial-intelligence-act/)
- [2025 AI Legislation - NCSL](https://www.ncsl.org/technology-and-communication/artificial-intelligence-2025-legislation)
- [2026 Federal, State, and EU AI Laws Guide](https://thenewstack.io/a-field-guide-to-2026-federal-state-and-eu-ai-laws/)
- [2026 AI Legal Forecast - CPO Magazine](https://www.cpomagazine.com/data-protection/2026-ai-legal-forecast-from-innovation-to-compliance/)

### Market and Industry Analysis
- [How 2026 Could Decide the Future of AI - CFR](https://www.cfr.org/articles/how-2026-could-decide-future-artificial-intelligence)
- [AI Agents Arrived in 2025 - The Conversation](https://theconversation.com/ai-agents-arrived-in-2025-heres-what-happened-and-the-challenges-ahead-in-2026-272325)
- [2026 Predictions for Autonomous AI - Palo Alto Networks](https://www.paloaltonetworks.com/blog/2025/11/2026-predictions-for-autonomous-ai/)
- [AI Governance Platforms - Gartner](https://www.gartner.com/reviews/market/ai-governance-platforms)
